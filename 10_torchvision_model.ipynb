{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b31217-2595-489d-9d41-38a7a1d8426a",
   "metadata": {},
   "source": [
    "# Torchvision Model (misas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a4609-1edd-4e1c-8653-13409190fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [[ -e /content ]] && git clone https://github.com/chfc-cmi/misas && cd misas && pip install .\n",
    "import os\n",
    "if os.path.isdir('/content/misas'):\n",
    "    os.chdir('/content/misas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675b028-d70b-45cf-b6ec-b9ed29380a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp torchvision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e923cd-cc7a-4f2f-b667-e9348c0bf941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow.compat.v1 as tf\n",
    "from PIL import Image, ImageEnhance, ImageShow, ImageOps\n",
    "import numpy as np\n",
    "import math\n",
    "#import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829da10-3721-475b-813e-8fac3f891c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def crop_pad_pil (image, size):\n",
    "    to_cut_w=((image.size[0]-size[0])/2)\n",
    "    to_cut_h=((image.size[1]-size[1])/2)\n",
    "    image = ImageOps.crop(image, (np.floor(to_cut_w), np.floor(to_cut_h), np.ceil(to_cut_w), np.ceil(to_cut_h)))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82493353-c401-4e03-b40a-ad619c2758d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ukbb_model:\n",
    "    def __init__(self, model_path):\n",
    "        tf.disable_eager_execution()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.import_meta_graph(f'{model_path}.meta')\n",
    "        saver.restore(self.sess, model_path)\n",
    "        \n",
    "    def prepareSize(self, image):\n",
    "        X, Y = image.size\n",
    "        image=crop_pad_pil(image,(int(math.ceil(X / 16.0)) * 16, int(math.ceil(Y / 16.0)) * 16))\n",
    "        return image\n",
    "    \n",
    "    def image_to_input(self, image):\n",
    "        img = image\n",
    "        img = np.array(img)[:,:,0]\n",
    "        img = img/255\n",
    "        img = np.expand_dims(img, 0)\n",
    "        img = np.expand_dims(img, -1)\n",
    "        return img\n",
    "    \n",
    "    def predict(self, image):\n",
    "        image = self.image_to_input(image)\n",
    "        preds, classes = self.sess.run(['prob:0', 'pred:0'],\n",
    "                   feed_dict={'image:0': image, 'training:0': False})\n",
    "        preds = np.squeeze(preds, 0)\n",
    "        preds = np.argmax(preds, axis = 2)\n",
    "        preds = preds.astype(np.uint8)\n",
    "        preds = Image.fromarray(preds)\n",
    "        #classes = ImageSegment(ByteTensor(classes))\n",
    "        return preds #classes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677c8ea-c5b4-4a14-810e-93bab32f0cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c6f5d-46d3-4987-801e-0281df260fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
