[
  {
    "objectID": "transversals.html",
    "href": "transversals.html",
    "title": "Case Study - Model Robustness",
    "section": "",
    "text": "We often find ourselves in a situation where we have trained a model for a certain task. The network performs well on the training and validation data. It also performs good on the hold out test set. Still you wonder what happens when you feed it data that is systematically different from all three sets? Does it make sense to re-train with more extensive data augmentation?\nIn this case study we demonstrate how misas helps answer these questions with a concrete example: - Model: Custom U-Net - Data: Small set of transversal CMR images"
  },
  {
    "objectID": "transversals.html#prepare-model-for-misas",
    "href": "transversals.html#prepare-model-for-misas",
    "title": "Case Study - Model Robustness",
    "section": "Prepare Model for misas",
    "text": "Prepare Model for misas\n\nfrom misas.core import default_cmap\nfrom misas.fastai_model import Fastai2_model\n\nfrom PIL import Image\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport altair as alt\n\n\ntrainedModel = Fastai2_model('chfc-cmi/transversal-cmr-seg', 'b0_transversal_5_5', force_reload=False)\n\nUsing cache found in /home/markus/.cache/torch/hub/chfc-cmi_transversal-cmr-seg_master"
  },
  {
    "objectID": "transversals.html#prepare-dataset-for-misas",
    "href": "transversals.html#prepare-dataset-for-misas",
    "title": "Case Study - Model Robustness",
    "section": "Prepare Dataset for misas",
    "text": "Prepare Dataset for misas\nData is available as png images and masks which is just fine for misas\n\nimg = Image.open(\"example/b0/images/train_example.png\").convert(\"RGB\")\nplt.imshow(img)\nimg.size\n\n(128, 128)"
  },
  {
    "objectID": "transversals.html#how-does-the-trained-model-perform-on-this-training-example",
    "href": "transversals.html#how-does-the-trained-model-perform-on-this-training-example",
    "title": "Case Study - Model Robustness",
    "section": "How does the trained model perform on this (training) example?",
    "text": "How does the trained model perform on this (training) example?\nTime to apply the model to the example image and see how it works (we need to call prepareSize manually here):\n\nimg = trainedModel.prepareSize(img)\nfig,ax = plt.subplots(figsize=(4,4))\nplt.imshow(img)\n_ = plt.imshow(trainedModel.predict(img), cmap=default_cmap, alpha=.5, interpolation=\"nearest\")\n\n\n\n\nSo how does it perform on validation data?\n\nfrom glob import glob\nfiles = sorted(glob(\"example/b0/images/val*.png\"))\n\n\nfig, axs = plt.subplots(1,5, figsize=(20,10))\nfor i, ax in enumerate(axs.flatten()):\n    fname = files[i]\n    tmp = trainedModel.prepareSize(Image.open(fname).convert(\"RGB\"))\n    ax.imshow(tmp)\n    ax.imshow(trainedModel.predict(tmp), cmap=default_cmap, alpha=.5, interpolation=\"nearest\")\n\n\n\n\nThis is not great. But given the limited training data it looks decent. So let’s have a closer look on how robust this model is. In particular to differences we might encounter when applying this network to new data."
  },
  {
    "objectID": "transversals.html#robustness-to-basic-transformations",
    "href": "transversals.html#robustness-to-basic-transformations",
    "title": "Case Study - Model Robustness",
    "section": "Robustness to basic transformations",
    "text": "Robustness to basic transformations\n\nfrom misas.core import *\n\n\nimg = lambda: Image.open(files[0]).convert(\"RGB\")\ntrueMask = lambda: Image.open(files[0].replace(\"image\",\"mask\")).convert(\"I\")\n\n\nSensitivity to orientation\nChanges in orientation are very common. Not because it is common to acquire images in different orientation but because the way data is stored in different file formats like nifti and dicom differs. So it is interesting to see how the model works in all possible orientations (including flips).\n\nplot_series(get_dihedral_series(img(),trainedModel), nrow=2, figsize=(20,12))\n\n\n\n\n\n\n\n\nresults = eval_dihedral_series(img(),trueMask(),trainedModel,components=[\"bg\",\"LV\",\"MY\"])\nresults\n\n\n\n\n\n\n\n\n  \n    \n      \n      k\n      bg\n      LV\n      MY\n    \n  \n  \n    \n      0\n      0\n      0.995824\n      0.782609\n      0.765604\n    \n    \n      1\n      1\n      0.963374\n      0.000000\n      0.000000\n    \n    \n      2\n      2\n      0.977227\n      0.000000\n      0.029690\n    \n    \n      3\n      3\n      0.985537\n      0.657269\n      0.206577\n    \n    \n      4\n      4\n      0.979158\n      0.000000\n      0.000000\n    \n    \n      5\n      5\n      0.987976\n      0.625101\n      0.439119\n    \n    \n      6\n      6\n      0.979158\n      0.000000\n      0.000000\n    \n    \n      7\n      7\n      0.992134\n      0.728850\n      0.640967\n    \n  \n\n\n\n\nNot surprisingly, the model is very sensitive to changes in orientation. So when using this model it is very important to feed the images in the proper orientation.\nAnother really interesting thing is that the heart is never properly segmented when images are flipped horizontally, so the heart is on the left instead of the right side. This gives a strong indication that the location within the image is one of the features the network has learned. Depending on your use case this might indeed be a sensible feature to use for segmentation of the heart as in a huge majority of cases the left ventricle of the heart is on the left side of the chest (so showing up on the right side in transversal slices).\n\n\nSensitivity to rotation\nThere should not be a huge variation in rotation (by small angles) when working with transversal slices. Still it is a good idea to get an impression of how quickly segmentation performance decreases with deviations in rotation.\n\nplot_series(get_rotation_series(img(),trainedModel, step=30, end=360), nrow=2)\n\n\n\n\n\n\n\n\nresults = eval_rotation_series(img(),trueMask(),trainedModel,start=-180,end=180,components=[\"bg\",\"LV\",\"MY\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['deg'],value_vars=['LV','MY']))\n .mark_line()\n .encode(\n     x=alt.X(\"deg\",axis=alt.Axis(title=None)),\n     y=alt.Y(\"value\",axis=alt.Axis(title=None)),\n     color=alt.Color(\"variable\",legend=None),\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\nSo there is quite a range (from -40 to 80 degrees) where prediction performance remains stable. This is sufficient not to worry about minor deviations.\nLet’s have another look at the network moving to the wrong side of thorax when predicting on rotated images:\n\ngif_series(\n    get_rotation_series(img(),trainedModel, start=0, end=360,step=10),\n    \"example/b0/rotation.gif\",\n    param_name=\"deg\",\n    duration=400\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to rotation\n\n\n\n\nSensitivity to cropping\nAnother variation that might occur in real life is a difference in field of view. This can happen due to different settings when acquiring the images or due to pre-processing steps in an analysis pipeline.\n\nplot_series(get_crop_series(img(),trainedModel, start = 5, end = 60, step = 10), nrow=2)\n\n\n\n\n\n\n\n\ngif_series(\n    get_crop_series(img(),trainedModel, start=60, end=5, step=-5),\n    \"example/b0/crop.gif\",\n    param_name=\"pixels\",\n    duration=400\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to rotation\n\n\nThis looks quite good. It seems to be okay to crop the image as long as the whole heart remains intact. As soon as we start to crop part of the heart the model is no longer able to find it (this is expected). It also does not start to predict heart somewhere, where it should not when cropping even further.\n\nresults = eval_crop_series(img(),trueMask(),trainedModel,start = 5, components=[\"bg\",\"LV\",\"MY\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['pixels'],value_vars=['LV','MY']))\n .mark_line()\n .encode(\n     x=\"pixels\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\nThe dice scores of 1 for very small sizes is because the model is not supposed to predict anything and it is not predicting anything. It drops to 0 when the heart starts to appear on the image but the model is still unable to locate it and then raises to the final performance it has on the whole image. Reaching a plateau at a size of 160px.\n\n\nSensitivity to brightness\n\nplot_series(get_brightness_series(img(),trainedModel, start=np.sqrt(2)/8), nrow=2)\n\n\n\n\n\n\n\n\nresults = eval_bright_series(img(),trueMask(),trainedModel, components=[\"bg\",\"LV\",\"MY\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['brightness'],value_vars=['LV','MY']))\n .mark_line()\n .encode(\n     x=\"brightness\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\n\n\nSensitivity to contrast\n\nplot_series(get_contrast_series(img(),trainedModel, start=1/8, end=np.sqrt(2)*8), nrow = 2)\n\n\n\n\n\n\n\n\nresults = eval_contrast_series(img(),trueMask(),trainedModel, components=[\"bg\",\"LV\",\"MY\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['contrast'],value_vars=['LV','MY']))\n .mark_line(point=True)\n .encode(\n     x=alt.X(\n     \"contrast\",\n     scale=alt.Scale(type=\"log\")),\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\",\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\n\n\nSensitivity to zoom\n\nplot_series(get_zoom_series(img(),trainedModel))\n\n\n\n\n\n\n\n\nresults = eval_zoom_series(img(),trueMask(),trainedModel,components=[\"bg\",\"LV\",\"MY\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['scale'],value_vars=['LV','MY']))\n .mark_line()\n .encode(\n     x=\"scale\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\n\ngif_series(\n    get_zoom_series(img(),trainedModel),\n    \"example/b0/zoom.gif\",\n    param_name=\"scale\",\n    duration=400\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to zoom"
  },
  {
    "objectID": "transversals.html#robustness-to-mr-artifacts",
    "href": "transversals.html#robustness-to-mr-artifacts",
    "title": "Case Study - Model Robustness",
    "section": "Robustness to MR artifacts",
    "text": "Robustness to MR artifacts\n\nSpike artifact\nSpike artifacts can happen with different intensities and at different locations in k-space. It is even possible to have multiple spikes.\n\nfrom misas.mri import *\n\nFirst we consider a single spike quite far from the center of k-space.\n\nplot_series(get_spike_series(img(),trainedModel))\n\n\n\n\n\n\n\nSegmentation performance is heavily impacted by this kind of artifact. The training examples did not have a single example with this herringbone pattern so we even get striped predictions for the myocardium.\nLet’s have a look how the location of the spike in k-space changes the artifact and model performance. From top to bottom, moving farter from the center.\n\nfor i in [.51,.55,.6,.75]:\n    plot_series(get_spike_series(img(),trainedModel,spikePosition=[i,i]), param_name=\"intensity\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo spikes closer to the center of k-space create artifacts with lower frequency and have less severe impact on segmentation performance.\nSo far we only looked at spikes on the diagonal of k-space, for the sake of completeness we can also look at arbitrary locations in k-space.\n\nfig, axs = plt.subplots(7,7,figsize=(16,16))\nvalues = [.4,.45,.49,.5,.51,.55,.6]\nvalues=list(itertools.product(values, values))\nfor x, ax in zip(values, axs.flatten()):\n    im = trainedModel.prepareSize(img())\n    im = spikeTransform(im,.5,list(x))\n    ax.imshow(np.array(im))\n    ax.imshow(np.array(trainedModel.predict(im)), cmap=default_cmap, alpha=.5, interpolation=\"nearest\")\n    ax.axes.xaxis.set_visible(False)\n    ax.axes.yaxis.set_visible(False)\n\n\n\n\n\n\n\\(B_0\\)-Field inhomogeneity\n\\(B_0\\)-Field inhomogeneieties are quite common, particularly at higher field strength. Adjusting these inhomogeneities at ultra high field strength (shimming) is an active field of research (Hock et al. 2020). So what is the impact of this so-called Bias field:\n\nplot_series(get_biasfield_series(img(),trainedModel))\n\n\n\n\n\n\n\nThe model works (at least on this example) reliably for even very intense field inhomogeneities."
  },
  {
    "objectID": "ukbb_on_kaggle.html",
    "href": "ukbb_on_kaggle.html",
    "title": "Case Study - Model Suitability",
    "section": "",
    "text": "We often find ourselves in a situation where we have a pre-trained model for a certain task (e.g. cardiac segmentation) and we have a data set where we want to perform that task. We know the model was not trained on that specific dataset. So we wonder, how will the model perform? Is it usable at all? Do we need to pre-process our data in a certain way to use it?\nIn this case study we demonstrate how misas helps answer these questions with a concrete example: - Model: ukbb_cardiac network by Bai et al. 2018 [1], trained on UK Biobank cardiac MRI images - Data: Kaggle Data Science Bowl Cardiac Challenge Data MRI images"
  },
  {
    "objectID": "ukbb_on_kaggle.html#prepare-model-for-misas",
    "href": "ukbb_on_kaggle.html#prepare-model-for-misas",
    "title": "Case Study - Model Suitability",
    "section": "Prepare Model for misas",
    "text": "Prepare Model for misas\nThe used model was trained on UK Biobank cardiac imaging data to segment short-axis images of the heart into left ventricle (LV), right ventricle (RV) and myocardium (MY). For details about the model please read the paper (Bai et al. 2018) and cite it if you use it. For implementation, training and usage see the GitHub repository. We downloaded the pre-trained model for short-axis images from https://www.doc.ic.ac.uk/~wbai/data/ukbb_cardiac/trained_model/ (local copy in example/kaggle/FCN_sa). In order to use it with misas we need to wrap it in a class that implements the desired interface (prepareSize and predict taking Image as input, see the main docu for more details).\nukbb_cardiac is written in tensorflow v1. With tensorflow v2 make sure to import the compat module.\n\nfrom misas.tensorflow_model import ukbb_model, crop_pad_pil\n\n\nmodel = ukbb_model('example/kaggle/FCN_sa')\n\nINFO:tensorflow:Restoring parameters from example/kaggle/FCN_sa\n\n\nThe model requires images to be a multiple of 16 in each dimension. We pad images accordingly in prepareSize. Additionally code in image_to_input takes care of the specifics of transforming a three-channel image into a single-item batch of single-channel images. In predict the output is converted to ImageSegment class."
  },
  {
    "objectID": "ukbb_on_kaggle.html#prepare-dataset-for-misas",
    "href": "ukbb_on_kaggle.html#prepare-dataset-for-misas",
    "title": "Case Study - Model Suitability",
    "section": "Prepare Dataset for misas",
    "text": "Prepare Dataset for misas\nThe Data Science Bowl Cardiac Challenge Data consists of MRI cine images from 1140 patients in dicom format. Multiple slices in short axis are available for each patient. Additionally, end-systolic and end-diastolic volumes are given (the original Kaggle challenge asked participants to predict these from the images).\nYou can download and unpack the dataset from the above website. Some example images are included in the example/kaggle/dicom folder.\nWe use pydicom to read the images and convert them to pillow Image objects.\n\nfrom pydicom import dcmread\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom misas.core import default_cmap, default_cmap_true_mask\n#import warnings\n#warnings.filterwarnings('ignore')\n\nWe use the window information within the dicom file to scale pixel intensities accordingly.\n\ndef prepareImage(fname):\n    ds = dcmread(fname)\n    img = (ds.pixel_array.astype(np.int16))\n    img = (img/img.max()*255).astype(np.uint8)\n    img = Image.fromarray(np.array(img))\n    return img.convert(\"RGB\")\n\nOkay, let’s look at an example:\n\nimg = prepareImage(\"example/kaggle/117_sax_76_IM-11654-0005.dcm\")\nplt.imshow(img)\nprint(np.array(img).shape)\n\n(256, 192, 3)"
  },
  {
    "objectID": "ukbb_on_kaggle.html#how-does-the-model-perform-out-of-the-box",
    "href": "ukbb_on_kaggle.html#how-does-the-model-perform-out-of-the-box",
    "title": "Case Study - Model Suitability",
    "section": "How does the model perform out of the box?",
    "text": "How does the model perform out of the box?\nTime to apply the model to the example image and see how it works (we need to call prepareSize manually here):\n\nimg = model.prepareSize(img)\nfig,ax = plt.subplots(figsize=(8,8))\nplt.imshow(img)\npred=model.predict(img)\nplt.imshow(pred, cmap=default_cmap, alpha=.5, interpolation=\"nearest\")\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\nThe model identified the left ventricle and myocardium partially. It failed to identify the right ventricle. So this result is promising in that it shows some kind of success but it is not usable as it is.\n\n\n\n\n\n\nNote\n\n\n\nThis is neither surprising nor a critique on the ukbb_cardiac network. That network was trained specifically for UK Biobank images and not to be applied generally.\n\n\nStill, we might be able to use it on the kaggle dataset if we understand why it fails and properly pre-process the data. This is where misas comes in. But first look at some more examples to see if we selected a bad example by chance.\n\nfrom glob import glob\n\n\ndcm_files = glob(\"example/kaggle/sample_images/*.dcm\")\nfig, axs = plt.subplots(2,5, figsize=(20,10))\nfor i, ax in enumerate(axs.flatten()):\n    tmp = model.prepareSize(prepareImage(dcm_files[i]))\n    ax.imshow(tmp)\n    #model.predict(tmp)[0].show(ax=ax, cmap=default_cmap)\n    ax.imshow(model.predict(tmp), cmap=default_cmap, vmax=3, alpha=.5, interpolation=\"nearest\")\n\n\n\n\nApparently our first impression that it does not work out of the box applies to most images. This also shows a bit of the variety of images and quality and also some inconsistencies in orientation."
  },
  {
    "objectID": "ukbb_on_kaggle.html#analysis-of-one-image",
    "href": "ukbb_on_kaggle.html#analysis-of-one-image",
    "title": "Case Study - Model Suitability",
    "section": "Analysis of one image",
    "text": "Analysis of one image\nIn order to get more detailed insights into what’s happening we select a specific example and define two helper functions that will open the image and true mask for that example. We use functions to get the image from file again instead of loading the image once and passing it around to avoid working with an accidentally modified version of the image.\nIn this case we have the true mask from a previous experiment but it would be possible to create that mask manually as well, it just needs to be saved as png with pixel values 0 for background and 1 to n for the n classes. As there are only three clases in this case looking at the png image in a standard image viewer will look like a purely black image.\n\nfrom misas.core import *\n\n\ndef img():\n    \"\"\"\n    Opens the sample image as a PIL image\n    \"\"\"\n    return Image.open(\"example/kaggle/images/1-frame014-slice006.png\").convert(\"RGB\")\n\ndef trueMask():\n    \"\"\"\n    Opens the true mask as a PIL image\n    \"\"\"\n    return Image.open(\"example/kaggle/masks_full/1-frame014-slice006.png\").convert(\"I\")\n\n\nSensitivity to orientation\nThere are eight posible orientations an image can be in (four rotations by 90° and a flipped variant for each). In misas a series with these 8 items is available via get_dihedral_series:\n\ndihedral_series = get_dihedral_series(img(),model, truth=trueMask())\n\n\n\n\n\nplot_series(dihedral_series, nrow=2, figsize=(20,12), param_name=\"orientation\", overlay_truth = True)\n\n\n\n\nIn the default orientation (0) the network is not successful, however when rotated by 90° clockwise (orientation 7) the prediction looks perfect. This can be explained by the different ways the pixel data is stored in the NifTi and DICOM formats. See this page to learn more.\nAnyway, as we now know, that (at least for this image) applying a dihedral transformation with parameter 7 yields optimal results we can include this transformation into the preparation function. This way aditional transformations will already use the correctly oriented image as starting point.\n\ndef prep_with_dihedral(image):\n    X, Y = image.size\n    image=crop_pad_pil(image,(int(np.ceil(X / 16.0)) * 16, int(np.ceil(Y / 16.0)) * 16))\n    return dihedralTransform(image, 6)\n\n\nmodel.prepareSize = prep_with_dihedral\n\n\n\nSensitivity to resize\nThe next thing that comes to mind when exploring the images from the kaggle dataset is that image size varies (120px-736px). Being fully convolutional, the ukbb_cardiac modell can handle various sizes. Still kernels are trained to work with features of a certain size in pixels so the network cannot be expected to work well with images scaled to arbitrary sizes. Let’s explore:\n\nplot_series(get_resize_series(img(),model, start=50, end=401,step=50), sharex=True, sharey=True, figsize=(20,10), nrow=2)\n\n\n\n\n\n\n\nIn fact predictions are very good even for small images (100px), they fail on very small images (50px). For large images, performance starts to get worse for right ventricle with images of 350px and for all other classes at 400px. Given the size range of the images in the dataset we have to worry about some of the larger images. But first we want to get a more quantitative view of the performance depending on size. So we can use the ground truth for the first time to calculate dice scores for each class and each parameter value. This can then be visualized as a line graph:\n\nresults = eval_resize_series(img(),trueMask(),model,end=600,step=30, components=[\"bg\",\"LV\",\"MY\",\"RV\"])\n\n\n\n\n\nimport altair as alt\n\n\n(alt\n .Chart(results.melt(id_vars=['px'],value_vars=['LV','MY','RV'],value_name='dice score'))\n .mark_line()\n .encode(\n     x=\"px\",\n     y=\"dice score\",\n     color=\"variable\",\n     tooltip=\"dice score\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\nApparently, there is quite a suitable size range. A size of 256px is well within that range so we can include a resize as part of the preparation:\n\ndef prep_with_dihedral_and_resize(image):\n    image = prep_with_dihedral(image)\n    return image.resize((256,256))\n\n\nmodel.prepareSize = prep_with_dihedral_and_resize\n\n\n\nSensitivity to rotation\nWe already know that orientation is really important. So we might wonder how much rotation will be tolerated. Is a rotation by 5° already a problem or will 30° still be fine? We use the same methods as before to address this question:\n\nplot_series(get_rotation_series(img(),model, step=30), nrow=2)\n\n\n\n\n\n\n\n\nresults = eval_rotation_series(img(),trueMask(),model,start=-180,end=180,components=[\"bg\",\"LV\",\"MY\",\"RV\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['deg'],value_vars=['LV','MY','RV'],value_name='dice score'))\n .mark_line()\n .encode(\n     x=\"deg\",\n     y=\"dice score\",\n     color=\"variable\",\n     tooltip=\"dice score\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\nThere is actually quite some tolerance to rotation. But it is not equal in both directions. Rotations by -80° are no problem for this particular image but only rotations up to +40° are possible without loss in performance.\nAnother tool misas provides to see the effect of a transformation on the prediction more vividly is using gifs:\n\ngif_series(\n    get_rotation_series(img(),model, start=0, end=360,step=10),\n    \"example/kaggle/rotation_ukbb.gif\",\n    param_name=\"deg\",\n    duration=400\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to rotation\n\n\n\n\nSensitivity to cropping\nNext up is the question of what features the network uses to make predictions? Does it use local features only or does it use the larger context, e.g. surrounding organs? As long as the region of interest is at the center of the image we can answer this question by successively cropping more and more contend from the border:\n\nplot_series(get_crop_series(img(),model, start = 0, step = 10, end=120), nrow=2)\n\n\nresults = eval_crop_series(img(),trueMask(),model,start = 5, components=[\"bg\",\"LV\",\"MY\",\"RV\"])\n\n\n(alt\n .Chart(results.melt(id_vars=['pixels'],value_vars=['LV','MY','RV'],value_name='dice score'))\n .mark_line()\n .encode(\n     x=\"pixels\",\n     y=\"dice score\",\n     color=\"variable\",\n     tooltip=\"dice score\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\nThis analysis indicates that not a lot of context is required for the network to reliably detect the heart (as long as at least the full left ventricle is part of the image). The dice score of 1.0 for MY and RV for very small sizes is due to the fact that the model does not predict anything and indeed there is no MY or RV left on the image. It remains 0.0 for LV as there is always LV on the center crop.\n\ngif_series(\n    get_crop_series(img(),model, start=5, end=120,step=10),\n    \"example/kaggle/crop_ukbb.gif\",\n    param_name=\"pixels\",\n    duration=400\n)\n\n\n\n\nsegmentation sensitivity to rotation\n\n\n\n\nSensitivity to brightness\nNext let’s see if the network is very sensitive to brightnes or contrast which would suggest some kind of pre processing, e.g. adaptive histogram equalization\n\nplot_series(get_brightness_series(img(),model, start=0.25, end=4*np.sqrt(2), step=np.sqrt(2),log_steps = True), nrow=2, figsize=(12,6)) #nrow=2)\n\n\nresults = eval_bright_series(img(),trueMask(),model, end = 8, components=[\"bg\",\"LV\",\"MY\",\"RV\"])\n\n\n(alt\n .Chart(results.melt(id_vars=['brightness'],value_vars=['LV','MY','RV'],value_name='dice score'))\n .mark_line()\n .encode(\n     x=\"brightness\",\n     y=\"dice score\",\n     color=\"variable\",\n     tooltip=\"dice score\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\ngif_series(\n    get_brightness_series(img(),model, start=0.25, end=8, step=np.sqrt(2),log_steps = True),\n    \"example/kaggle/bright_ukbb.gif\",\n    param_name=\"brightness\",\n    duration=400\n)\n\n\n\n\nsensitivity to brightness\n\n\nThe network seems to be quite robust to differences in brightness.\n\n\nSensitivity to contrast\n\nplot_series(get_contrast_series(img(),model, start=0.25, end=8, step=np.sqrt(2),log_steps = True))\n\n\nresults = eval_contrast_series(img(),trueMask(),model, end = 2.5, step= 0.3, components=[\"bg\",\"LV\",\"MY\",\"RV\"])\n\n\n(alt\n .Chart(results.melt(id_vars=['contrast'],value_vars=['LV','MY','RV'],value_name='dice score'))\n .mark_line()\n .encode(\n     x=\"contrast\",\n     y=\"dice score\",\n     color=\"variable\",\n     tooltip=\"dice score\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\ngif_series(\n    get_contrast_series(img(),model, start=0.25, end=8, step=np.sqrt(2),log_steps = True),\n    \"example/kaggle/contrast_ukbb.gif\",\n    param_name=\"contrast\",\n    duration=400\n)\n\n\n\n\nsegmentation sensitivity to contrast\n\n\nThe network seems to be quite robust to differences in contrast."
  },
  {
    "objectID": "ukbb_on_kaggle.html#references",
    "href": "ukbb_on_kaggle.html#references",
    "title": "Case Study - Model Suitability",
    "section": "References",
    "text": "References\n[1] W. Bai, et al. Automated cardiovascular magnetic resonance image analysis with fully convolutional networks. Journal of Cardiovascular Magnetic Resonance, 20:65, 2018."
  },
  {
    "objectID": "ukbb_on_kaggle.html#supplemental-information",
    "href": "ukbb_on_kaggle.html#supplemental-information",
    "title": "Case Study - Model Suitability",
    "section": "Supplemental Information",
    "text": "Supplemental Information\nIf you have the full kaggle dataset you can draw a random sample using this code (omit the seed to really make it random)\n#nbdev_fulldata_test\n# As we cannot include the whole kaggle dataset in the repo I draw a sample like this:\nfrom glob import glob\nfrom shutil import copy\nimport os\nimport random\n\nrandom.seed(42)\ndcm_files = glob(\"kaggle/train/*/sax_*/*.dcm\")\nos.makedirs('example/kaggle/sample_images', exist_ok=True)\nfor f in random.sample(dcm_files,20):\n    copy(f, 'example/kaggle/sample_images')"
  },
  {
    "objectID": "mr_artifacts.html",
    "href": "mr_artifacts.html",
    "title": "Simulated MR artifacts (torchio)",
    "section": "",
    "text": "import fastai.medical.imaging\nIf you use these transformations which all build on TorchIO, please cite the following paper: > Pérez-García et al., TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. Link: https://arxiv.org/abs/2003.04696"
  },
  {
    "objectID": "mr_artifacts.html#spike-artifact",
    "href": "mr_artifacts.html#spike-artifact",
    "title": "Simulated MR artifacts (torchio)",
    "section": "Spike artifact",
    "text": "Spike artifact\n\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n\nsource\n\nget_spike_series\n\n get_spike_series (image, model, start=0, end=2.5, step=0.5,\n                   spikePosition=[0.1, 0.1], **kwargs)\n\n\nsource\n\n\nspikeTransform\n\n spikeTransform (image, intensityFactor, spikePosition=[0.1, 0.1])\n\n\nfname = \"example/b0/images/val_example_0.png\"\nimg = lambda: Image.open(fname).resize((256,256)).convert (\"RGB\")\ntrueMask = lambda: Image.open(fname.replace(\"image\",\"mask\"))\n\n\nspikeTransform(img(), 1, spikePosition=[.5,.5])\n\n\n\n\n\ndef interactiveSpikeTransform(intensity, x, y):\n    plt.imshow(spikeTransform(img(), intensity, [x,y])) #figsize=(8,8))\n\n\ninteractive_plot = interactive(\n    interactiveSpikeTransform,\n    intensity=widgets.FloatSlider(min=0, max=3, step=.1, value=1, continuous_update=True),\n    x=widgets.FloatSlider(min=0, max=.99, step=.05, value=.5, continuous_update=True),\n    y=widgets.FloatSlider(min=0, max=.99, step=.05, value=.5, continuous_update=True)\n)\ninteractive_plot\n\n\n\n\n\nplt.imshow(spikeTransform(img(), .8, [.4,.3]), cmap=\"bone\") #figsize=(8,8),\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\ndef label_func(x):\n    pass\ndef acc_seg(input, target):\n    pass\ndef diceComb(input, targs):\n    pass\ndef diceLV(input, targs):\n    pass\ndef diceMY(input, targs):\n    pass\n\n\ntrainedModel = Fastai2_model('chfc-cmi/transversal-cmr-seg', 'b0_transversal_5_5', force_reload=False)\n\nUsing cache found in /home/markus/.cache/torch/hub/chfc-cmi_transversal-cmr-seg_master\n\n\n\n\nSpike intensity\n\nseries = get_spike_series(img(), trainedModel, truth=trueMask(), tfm_y=False)\n\n\n\n\n\nplot_series(series)\n\n\n\n\n\nspikePosition = [.45,.55]\nplot_series(get_spike_series(img(), trainedModel, truth=trueMask(), spikePosition=spikePosition))\n\n\n\n\n\n\n\n\nspikePosition = [.5,.52]\nplot_series(get_spike_series(img(), trainedModel, truth=trueMask(), spikePosition=spikePosition))\n\n\n\n\n\n\n\n\nsource\n\n\neval_spike_series\n\n eval_spike_series (image, mask, model, step=0.1, start=0, end=2.5,\n                    spikePosition=[0.1, 0.1], param_name='intensity',\n                    **kwargs)\n\n\nresults = eval_spike_series(img(), trueMask(), trainedModel)\nplt.plot(results['intensity'], results['c1'], label='LV')\nplt.plot(results['intensity'], results['c2'], label='MY')\nplt.legend()\nplt.xlabel('relative intensity')\nplt.ylabel('dice')\n_ = plt.title(f'spike position: {[.1,.1]}')\n\n\n\n\n\n\n\n\n\nSpike position\n\nsource\n\n\nget_spike_pos_series\n\n get_spike_pos_series (image, model, start=0.1, end=0.9, step=0.1,\n                       intensityFactor=0.5, spikePositionY=0.1, **kwargs)\n\n\nsource\n\n\nspikePosTransform\n\n spikePosTransform (image, spikePositionX, spikePositionY=0.1,\n                    intensityFactor=0.5)\n\n\nseries = get_spike_pos_series(img(), trainedModel, intensityFactor=0.1, spikePositionY=0.1)\nplot_series(series, nrow=1, figsize=(20,15), param_name='Spike X Position')\n\n\n\n\n\n\n\n\nsource\n\n\neval_spike_pos_series\n\n eval_spike_pos_series (image, mask, model, step=0.1, start=0.1, end=0.9,\n                        intensityFactor=0.1, param_name='Spike X\n                        Position', **kwargs)\n\n\nintensityFactor = 0.2\nresults = eval_spike_pos_series(img(), trueMask(), trainedModel, step=0.05, intensityFactor=intensityFactor)\nplt.plot(results['Spike X Position'], results['c1'], label='LV')\nplt.plot(results['Spike X Position'], results['c2'], label='MY')\nplt.legend()\nplt.xlabel('X position of the spike')\nplt.ylabel('Dice')\n_ = plt.title(f'Spike intensity = {intensityFactor}')\n\n\n\n\n\n\n\n\ngif_series(\n    get_spike_pos_series(img(),trainedModel, start=0.1, end=0.9 ,step=0.1, intensityFactor=0.2),\n    \"example/kaggle/spike_pos.gif\",\n    param_name=\"Spike X Position\",\n    duration=400\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to spike position"
  },
  {
    "objectID": "mr_artifacts.html#bias-field-b_0",
    "href": "mr_artifacts.html#bias-field-b_0",
    "title": "Simulated MR artifacts (torchio)",
    "section": "Bias Field (\\(B_0\\))",
    "text": "Bias Field (\\(B_0\\))\nThe bias field function is still experimental as it lacks configurability. To make it deterministic all coefficients are set to the exact same value. This represents only one specific possibility of field inhomogeneity.\n\nimage = img()\nimage = np.array(image)[:,:,0]\nimage = np.expand_dims(np.array(image), 0)\nimage = np.expand_dims(np.array(image), 0)\nimage.shape\n\n(1, 1, 256, 256)\n\n\n\ncoefficients = RandomBiasField().get_params(3,[-.3,-.3])\n\n\nbf = BiasField.generate_bias_field(image, order=3, coefficients=coefficients)\n\n\nplt.imshow(bf[0])\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nimage[0][0].shape\nimage = image/255\n\n\nimage = torch.from_numpy(image)\nresult = image[0][0] * bf[0]\nresult = torch.stack((result,result,result))\nresult = np.array((torch.clamp(result,0,1)))\nresult = result*255\nresult = result.astype(np.uint8)\nresult = np.moveaxis(result, 0, 2)\nresult = Image.fromarray(result)\nplt.imshow(result)\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nsource\n\nget_biasfield_series\n\n get_biasfield_series (image, model, start=0, end=-0.6, step=-0.2,\n                       order=3, **kwargs)\n\n\nsource\n\n\nbiasfieldTransform\n\n biasfieldTransform (image, coef, order=3)\n\n\nplot_series(get_biasfield_series(img(),trainedModel,step=-.1),nrow=1)\n\n\n\n\n\n\n\n\nbiasfieldTransform(img(),-.5)\n\n\n\n\n\nsource\n\n\neval_biasfield_series\n\n eval_biasfield_series (image, mask, model, step=-0.05, start=0,\n                        end=-0.55, order=3, **kwargs)\n\n\nresults = eval_biasfield_series(img(), trueMask(), trainedModel)\nplt.plot(results['coefficient'], results['c1'], label='LV')\nplt.plot(results['coefficient'], results['c2'], label='MY')\nplt.legend()\nplt.xlabel('coefficient')\n_ = plt.ylabel('dice')"
  },
  {
    "objectID": "pneumothorax.html",
    "href": "pneumothorax.html",
    "title": "Pneumothorax demo",
    "section": "",
    "text": "pneumothorax_source = untar_data(URLs.SIIM_SMALL)\n\n\nitems = get_dicom_files(pneumothorax_source/f\"train/\")\n\n\ndf = pd.read_csv(pneumothorax_source/f\"labels.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      file\n      label\n    \n  \n  \n    \n      0\n      train/No Pneumothorax/000000.dcm\n      No Pneumothorax\n    \n    \n      1\n      train/Pneumothorax/000001.dcm\n      Pneumothorax\n    \n    \n      2\n      train/No Pneumothorax/000002.dcm\n      No Pneumothorax\n    \n    \n      3\n      train/Pneumothorax/000003.dcm\n      Pneumothorax\n    \n    \n      4\n      train/Pneumothorax/000004.dcm\n      Pneumothorax\n    \n  \n\n\n\n\n\ndf_pneumo = df[df['label']=='Pneumothorax']\n\n\npneumothorax = DataBlock(blocks=(ImageBlock(cls=PILDicom), MaskBlock(codes=['bg','pneumo'])),\n                   get_x=lambda x:pneumothorax_source/f\"{x[0]}\",\n                   get_y=lambda x:'example/siim/'+Path(x[0]).stem+'.png',\n                   batch_tfms=aug_transforms(do_flip=False,p_affine=0,p_lighting=0,size=224))\n\ndls = pneumothorax.dataloaders(df_pneumo.values, num_workers=0, bs=16)\n\n\ndls.show_batch(max_n=16, vmin=0)\n\n\n\n\n\n# just for testing - you can skip this when running interactively\nlearn = unet_learner(dls, resnet34)\n\n\nlearn.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.382017\n      0.309487\n      01:04\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.167802\n      0.111701\n      00:51\n    \n  \n\n\n\n\n# this is skipped when testing, instead a single epoch is done (not sufficient for overfitting)\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(8)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.200016\n      0.416453\n      01:05\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.195540\n      0.105748\n      00:51\n    \n    \n      1\n      0.148728\n      0.141623\n      00:51\n    \n    \n      2\n      0.126741\n      0.176495\n      00:54\n    \n    \n      3\n      0.116757\n      0.106115\n      00:57\n    \n    \n      4\n      0.106740\n      0.093054\n      02:16\n    \n    \n      5\n      0.098731\n      0.140762\n      01:49\n    \n    \n      6\n      0.092059\n      0.120493\n      02:27\n    \n    \n      7\n      0.086470\n      0.116142\n      01:48\n    \n  \n\n\n\n\nlearn.show_results(ds_idx=0,max_n=9,vmin=0)\n\n\n\n\n\n\n\n\n\n\n\n\npreds = learn.get_preds(ds_idx=0)\n\n\n\n\n\n\n\n\n\nplt.imshow(preds[1][2])\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\npreds[0].argmax(dim=1).sum()\n#plt.imshow(preds[0].argmax(dim=1)[0])\n\nTensorBase(0)\n\n\n\nlearn.fine_tune(8)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.061920\n      2.473048\n      01:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.314660\n      0.080977\n      01:54\n    \n    \n      1\n      0.208264\n      0.088037\n      00:55\n    \n    \n      2\n      0.160613\n      0.078349\n      00:54\n    \n    \n      3\n      0.134872\n      0.086172\n      00:52\n    \n    \n      4\n      0.113790\n      0.082046\n      00:52\n    \n    \n      5\n      0.100933\n      0.077185\n      00:53\n    \n    \n      6\n      0.090635\n      0.089535\n      01:10\n    \n    \n      7\n      0.083492\n      0.089540\n      01:10\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\npreds = learn.get_preds(ds_idx=0)\npreds[0].argmax(dim=1).sum()\n\n\n\n\n\n\n\n\nTensorBase(0)\n\n\n\nfig, axs = plt.subplots(5,2,figsize=(8,16))\nfor i in range(5):\n    axs[i,0].imshow(preds[1][i])\n    axs[i,1].imshow(preds[0][i][1])\n    print(preds[0][i][1].max())\n\nTensorBase(0.3102)\nTensorBase(0.3205)\nTensorBase(0.3076)\nTensorBase(0.2694)\nTensorBase(0.0728)\n\n\n\n\n\n\nlearn.fit_one_cycle(8)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.043738\n      0.085497\n      01:06\n    \n    \n      1\n      0.041399\n      0.126955\n      00:53\n    \n    \n      2\n      0.039557\n      0.166106\n      00:52\n    \n    \n      3\n      0.038748\n      0.160928\n      00:55\n    \n    \n      4\n      0.036368\n      0.123756\n      00:52\n    \n    \n      5\n      0.034367\n      0.111612\n      00:52\n    \n    \n      6\n      0.032606\n      0.130679\n      00:52\n    \n    \n      7\n      0.030831\n      0.122198\n      00:53\n    \n  \n\n\n\n\npreds = learn.get_preds(ds_idx=0)\npreds[0].argmax(dim=1).sum()\n\n\n\n\n\n\n\n\nTensorBase(14126)\n\n\n\nlearn.fit_one_cycle(20)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.023082\n      0.114249\n      00:56\n    \n    \n      1\n      0.022736\n      0.110707\n      00:53\n    \n    \n      2\n      0.021745\n      0.209588\n      00:52\n    \n    \n      3\n      0.022693\n      0.105819\n      01:18\n    \n    \n      4\n      0.024324\n      0.151829\n      00:49\n    \n    \n      5\n      0.024436\n      0.093544\n      00:49\n    \n    \n      6\n      0.024203\n      0.123490\n      00:48\n    \n    \n      7\n      0.023524\n      0.166477\n      00:50\n    \n    \n      8\n      0.022349\n      0.262088\n      00:55\n    \n    \n      9\n      0.021310\n      0.254958\n      01:41\n    \n    \n      10\n      0.020904\n      0.199529\n      02:09\n    \n    \n      11\n      0.020301\n      0.199600\n      00:58\n    \n    \n      12\n      0.019917\n      0.186603\n      01:07\n    \n    \n      13\n      0.019124\n      0.135860\n      00:52\n    \n    \n      14\n      0.018472\n      0.152205\n      00:49\n    \n    \n      15\n      0.017754\n      0.178598\n      00:52\n    \n    \n      16\n      0.017116\n      0.187399\n      00:56\n    \n    \n      17\n      0.016518\n      0.185505\n      00:56\n    \n    \n      18\n      0.015890\n      0.184788\n      00:57\n    \n    \n      19\n      0.015337\n      0.181746\n      00:57\n    \n  \n\n\n\n\npreds = learn.get_preds(ds_idx=0)\npreds[0].argmax(dim=1).sum()\n\n\n\n\n\n\n\n\nTensorBase(41177)\n\n\n\nfig, axs = plt.subplots(5,3,figsize=(12,16))\nfor i in range(5):\n    axs[i,0].imshow(preds[1][i], interpolation=\"nearest\")\n    axs[i,1].imshow(preds[0][i][1])\n    axs[i,2].imshow(preds[0].argmax(dim=1)[i], interpolation=\"nearest\")\n\n\n\n\n\nlearn.save('pneumothorx')\n\nPath('models/pneumothorx.pth')\n\n\n\nWe managed to overfit\n\nlearn.load('pneumothorx')\n\n<fastai.learner.Learner>\n\n\n\nfrom misas.core import *\nfrom fastai.vision.all import * #open_mask, Image, ImageSegment\nimport pydicom\n\n\ndef read_dcm(file):\n    ds = pydicom.dcmread(file)\n    img = Tensor(ds.pixel_array.astype(np.int16))\n    img = img/img.max()\n    transform = t.ToPILImage()\n    img = transform (torch.stack([img, img, img]))\n    return img\n\n\nlearn.predict(learn.dls.train_ds.items[1])[0].show(vmin=0)\n\n\n\n\n\n\n\n\n<AxesSubplot:>\n\n\n\n\n\n\nfname = df_pneumo.iloc[3]['file']\nimg = lambda: read_dcm(pneumothorax_source/fname)\ntrueMask = lambda: Image.open('example/siim/'+Path(fname).stem+'.png') #open_mask('example/siim/'+Path(fname).stem+'.png')\n\n\ndcm = img()\n\n\ndcm\n#to_image(dcm.data)\n\n\n\n\n\npred = learn.predict(PILDicom(dcm))[1]\nplt.imshow(pred)\n\n\n\n\n\n\n\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nclass Fastai2_wrapper:\n    def __init__(self, model):\n        self.model = model\n        self.model.cbs = L([])\n    \n    def imageToPILDicom(self, image):\n        return PILDicom(image)\n    \n    def prepareSize(self, imageOrMask):\n        return imageOrMask.resize((224,224))\n    \n    def predict(self, image):\n        image = self.imageToPILDicom(image)\n        pred = self.model.predict(image)[0]#(in_image)\n        return Image.fromarray(np.array(pred).astype(np.uint8))\n\n\nmodel = Fastai2_wrapper(learn)\n\n\nplt.imshow(model.predict(img()))\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nplot_series(get_rotation_series(img(), model))\n\n\n\n\n\n\n\n\nresult = eval_rotation_series(img(), trueMask(), model, start=-180, end=180, components=['bg','pneumo'])\n\n\n\n\n\nplt.plot(result['deg'],result['pneumo'])"
  },
  {
    "objectID": "camvid.html",
    "href": "camvid.html",
    "title": "CamVid Demo",
    "section": "",
    "text": "Please reach out via GitHub if you have problems using misas on your own dataset.\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.CAMVID_TINY)\n\n\ncodes = np.loadtxt(path/'codes.txt', dtype=str)\ncodes\n\narray(['Animal', 'Archway', 'Bicyclist', 'Bridge', 'Building', 'Car',\n       'CartLuggagePram', 'Child', 'Column_Pole', 'Fence', 'LaneMkgsDriv',\n       'LaneMkgsNonDriv', 'Misc_Text', 'MotorcycleScooter', 'OtherMoving',\n       'ParkingBlock', 'Pedestrian', 'Road', 'RoadShoulder', 'Sidewalk',\n       'SignSymbol', 'Sky', 'SUVPickupTruck', 'TrafficCone',\n       'TrafficLight', 'Train', 'Tree', 'Truck_Bus', 'Tunnel',\n       'VegetationMisc', 'Void', 'Wall'], dtype='<U17')\n\n\n\nfnames = get_image_files(path/\"images\")\n\n\ndef label_func(fn): return path/\"labels\"/f\"{fn.stem}_P{fn.suffix}\"\n\n\ncam_fn = fnames[0]\nmask_fn = label_func(fnames[0])\n\ncam_img = lambda: Image.open(cam_fn).convert(\"RGB\")\nmask = lambda: Image.open(mask_fn).convert(\"I\")\n\n\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = fnames, label_func = label_func, codes = codes\n)\n\n\ndls.show_batch(max_n=6)\n\n\n\n\n\nlearn = unet_learner(dls, resnet34)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /home/markus/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlearn.fine_tune(6)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.004354\n      2.548569\n      00:30\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.048129\n      1.562803\n      00:37\n    \n    \n      1\n      1.704228\n      1.354810\n      00:33\n    \n    \n      2\n      1.483399\n      1.031199\n      00:30\n    \n    \n      3\n      1.315127\n      0.921900\n      00:27\n    \n    \n      4\n      1.164001\n      0.827559\n      00:33\n    \n    \n      5\n      1.051317\n      0.808568\n      00:32\n    \n  \n\n\n\n\nlearn.show_results(max_n=6, figsize=(7,8))\n\n\n\n\n\n\n\n\n\n\n\n\ninterp = SegmentationInterpretation.from_learner(learn)\ninterp.plot_top_losses(k=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncamvid = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),\n                   get_items = get_image_files,\n                   get_y = label_func,\n                   splitter=RandomSplitter(),\n                   batch_tfms=aug_transforms(size=(120,160)))\n\n\ndls = camvid.dataloaders(path/\"images\", path=path, bs=8)\n\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n# learn.save('mini_train')\n\n\nplt.imshow(cam_img().resize((128,128)))\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nfrom misas.core import *\nfrom misas.core import default_cmap\n\n\nlearn.prepareSize = lambda item: item.resize((128,128))\n\n\nclass Fastai_model:\n    def __init__(self, learner):\n        self.trainedModel = learner\n        self.resize128 = lambda x: x.resize ((128,128))\n        self.trainedModel.remove_cbs(ProgressCallback)\n\n        \n    def prepareSize(self, item):\n        return self.resize128(item)\n    \n    def predict(self, image):\n        image = PILImage.create(np.array(image))\n        output = self.trainedModel.predict(image)\n        output = PILImage.create(output [0])\n        output = Image.fromarray(np.array(output)) #mode=\"I\"\n        return output\n\n\nCam_vid = Fastai_model(learn)\n\n\nplot_series(get_rotation_series(cam_img(), Cam_vid), vmax=31, vmin=0)\n\n\n\n\n\n\n\n\nplot_series(get_zoom_series(cam_img(), Cam_vid), vmax=31, vmin=0, nrow=2)\n\n\n\n\n\n\n\n\nfound_classes = np.unique(np.array(Cam_vid.predict(cam_img())))\n\n\ncodes[found_classes]\n\narray(['Building', 'Car', 'LaneMkgsDriv', 'Road', 'Sidewalk', 'Sky',\n       'Tree', 'Void', 'Wall'], dtype='<U17')\n\n\n\nresult = eval_rotation_series(cam_img(),mask(),Cam_vid,components=codes)\n\n\n\n\nWhen plotting the evaluation series, it makes sense to only plot classes that actually occur.\n\nplot_eval_series(result[np.append(\"deg\",codes[found_classes])])"
  },
  {
    "objectID": "batch_mode.html",
    "href": "batch_mode.html",
    "title": "Batch Mode",
    "section": "",
    "text": "import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
  },
  {
    "objectID": "batch_mode.html#aim-getting-a-first-impression-on-the-models-performance-on-the-given-data-before-going-into-detailed-evaluation",
    "href": "batch_mode.html#aim-getting-a-first-impression-on-the-models-performance-on-the-given-data-before-going-into-detailed-evaluation",
    "title": "Batch Mode",
    "section": "Aim: Getting a first impression on the models performance on the given data before going into detailed evaluation",
    "text": "Aim: Getting a first impression on the models performance on the given data before going into detailed evaluation\nIn this case study we demonstrate how misas can give you an overview of the performance of the model by creating plots that show the average dice score of the whole batch of images over different parameters: - Model: ukbb_cardiac network by Bai et al. 2018 [1], trained on UK Biobank cardiac MRI images - Data: Kaggle Data Science Bowl Cardiac Challenge Data MRI images"
  },
  {
    "objectID": "batch_mode.html#prepare-model-for-misas",
    "href": "batch_mode.html#prepare-model-for-misas",
    "title": "Batch Mode",
    "section": "Prepare Model for Misas",
    "text": "Prepare Model for Misas\nThe used model was trained on UK Biobank cardiac imaging data to segment short-axis images of the heart into left ventricle (LV), right ventricle (RV) and myocardium (MY). For details about the model please read the paper (Bai et al. 2018) and cite it if you use it. For implementation, training and usage see the GitHub repository. We downloaded the pre-trained model for short-axis images from https://www.doc.ic.ac.uk/~wbai/data/ukbb_cardiac/trained_model/ (local copy in example/kaggle/FCN_sa). In order to use it with misas we need to wrap it in a class that implements the desired interface (prepareSize and predict taking Image as input, see the main docu for more details).\nukbb_cardiac is written in tensorflow v1. With tensorflow v2 make sure to import the compat module.\nThe model requires images to be a multiple of 16 in each dimension. We pad images accordingly in prepareSize. Additionally, code in dicom_to_Image takes care of the specifics of transforming a three-channel image into a single-item batch of single-channel images. In predict the output is converted to PIL Image class.\n\nfrom misas.tensorflow_model import ukbb_model, crop_pad_pil\n\n\nmodel = ukbb_model('example/kaggle/FCN_sa')\n\nINFO:tensorflow:Restoring parameters from example/kaggle/FCN_sa"
  },
  {
    "objectID": "batch_mode.html#prepare-images",
    "href": "batch_mode.html#prepare-images",
    "title": "Batch Mode",
    "section": "Prepare Images",
    "text": "Prepare Images\n\nFind proper Orientation\nWe found out, in notebook 02_ukbb_on_kaggle, that prediction is much better if images are prepared properly. However, the images used there, were already converted to png. Now we want to use raw dicoms. The scaling will likely still be the same but orientation might differ. So let’s check with the standard dihedral transformation.\n\nimage = \"example/kaggle/sample_images/IM-4562-0014.dcm\"\n\n\nplot_series(get_dihedral_series(dicom_to_Image(image),model))\n\n\n\n\n\n\n\n\ndef prep_with_dihedral_and_resize(image):\n    Y,X = image.size\n    image = crop_pad_pil(image,(int(np.ceil(X / 16.0)) * 16, int(np.ceil(Y / 16.0)) * 16))\n    image = dihedralTransform(image, 7)\n    return image.resize((256,256))\n\nmodel.prepareSize = prep_with_dihedral_and_resize\n\nFor these demonstration purposes, images are selected randomly from all images that have the attributes “Slice Location and”Trigger Time” and that are not automatically generated images that contain plain text or multiple images:\n\nimages = [\n    \"example/kaggle/sample_images/IM-13717-0026.dcm\",\n    \"example/kaggle/sample_images/IM-7453-0024.dcm\",\n    \"example/kaggle/sample_images/IM-4718-0021.dcm\",\n    \"example/kaggle/sample_images/IM-5022-0015.dcm\",\n    \"example/kaggle/sample_images/IM-14141-0011.dcm\",\n    \"example/kaggle/sample_images/IM-13811-0003.dcm\",\n    \"example/kaggle/sample_images/IM-4562-0014.dcm\"\n]"
  },
  {
    "objectID": "batch_mode.html#evaluation",
    "href": "batch_mode.html#evaluation",
    "title": "Batch Mode",
    "section": "Evaluation",
    "text": "Evaluation\nIf true_masks is “None”, the model will predict a mask for every image, save it as png in a newly created directory and use this mask as truth for the evaluation and afterwards delete the directory and the contained predicted thruths. Thus, it is important to hand in the images in an orientation in which the model makes good predictions, in this case this is handled, by the “prepareImage” function, which reads the dicom file, converts it to a fastai Image object, and rotates and flips the image into correct position\n\n\n\n\n\n\nWarning\n\n\n\nWhen mask_prepareSize is set to false, so when no true mask is provided by the user and thus generated by batch_mode, functions that change the size of the true mask like resize don’t work any longer.\n\n\n\nsource\n\nbatch_results\n\n batch_results (images, model, eval_functions, true_masks=None,\n                components=['bg', 'LV', 'MY', 'RV'])\n\nEvaluation of the models performance across multiple images and transformations\n-images (list): paths for dicom files with which the model should be evaluated -model: model to be evaluated -eval_functions (dictionary): keys: name of transformation, values: eval functions from misas.core -true_masks (list, optional): paths of png files with true masks for dicoms in ‘images’ in the same order as ‘images’ -components (list, optional): classes that will be evaluated by the eval functions\nReturns: list of Pandas dataFrames, that contains one dataFrame for each image with the columns: ‘parameter’, ‘bg’, ‘LV’, ‘MY’, ‘RV’, ‘File’\n\ntruths = []\nfor i in images:\n    img = dicom_to_Image(i)\n    # apply the prepareSize transformation\n    truth = model.predict(model.prepareSize(img))#[0]\n    # reverse the rotation from prepareSize as it will be re-applied on the mask before comparison\n    truth = dihedralTransform(truth, 7)\n    tmpfile = mkstemp()\n    truth.save(tmpfile[1] + \".png\")\n    truths.append(tmpfile[1] + \".png\")\n\nIn this example we are passing a list of true masks to the function that will be used for evaluation. Since this is only for demonstration purposes, we will manually generate the list of true masks by predicting the truth for every image and saving it to a temporary directory and storing the path names in a list, from where batch_results can access it.\n\nimg = dicom_to_Image(images[0])\ntruth = model.predict(model.prepareSize(img))#[0]\n#tmpfile = mkstemp()\n#truth.save(tmpfile[1] + \".png\")\n#truths.append(tmpfile[1] + \".png\")\n\nThe functions will be passed to batch_results() in a dictionary. The keys are the names of the respective transformations, which will also be used for the title of the final plots. The values are the eval_functions from misas.core. In this case we called them with partial() to edit the parameter nams and start/end/step parameters:\n\nresults_with_truths = batch_results(images, model, {\n                                    \"Rotation\": partial(eval_rotation_series, param_name=\"Degrees\"),\n                                    \"Brightness\": partial(eval_bright_series, param_name=\"Relative Brightness\", start=0.025, end=.975, step=0.025),\n                                    \"Cropping\": partial(eval_crop_series, param_name=\"Image size in Pixels\"),\n                                    \"Contrast\": partial(eval_contrast_series, param_name=\"Relative Contrast\", start=0.1, end=4.5, step=0.1),\n                                    \"Resizing\": partial(eval_resize_series, param_name=\"Pixels\", start=20, end=700, step=25),\n                                    \"Spike Artefact Intensity\": partial(eval_spike_series, param_name=\"Intensity\", step=.01, start=0, end=.5),\n                                    \"Spike Artefact Position\": partial(eval_spike_pos_series, param_name=\"X Position of Spike\", step=0.025, intensityFactor=0.2),\n                                    \"Zoom\": partial(eval_zoom_series, param_name=\"Scale\")\n                                    }, components=['bg','LV','MY','RV'], true_masks=truths)\ndihedral_results_with_truths = batch_results(images, model, {\"Orientation\": partial(eval_dihedral_series, param_name=\"Orientation\")}, components=['bg','LV','MY','RV'], true_masks=truths)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the dihedral transformation we are storing the evaluation in a separate variable, so we can create a plot that looks different from the others:\nIn the following we demonstrate that the function can also evaluate the models performance without passing a list of true masks to the function. In this case the model will make a prediction for every image, save it as png and use it as a true mask for evaluation. In this case it is important that the evaluated images are passed to the function in an orientation in which the model can make a good prediction, since this prediction is used as the true mask for all the following transformations. In this case the “right” orientation was evaluated by applying dihedral/rotation transformations to one single example image and evaluating the predictions with misas. Bringing all the images for batch evaluation into this desired orientation is in this case handled by the dicom_to_Image function, but it has to be manually adapted if a different set of data or a different model is used.\n\nresults_without_truths = batch_results(images, model, {\n                                    \"Rotation\": partial(eval_rotation_series, param_name=\"Degrees\"),\n                                    #\"Brightness\": partial(eval_bright_series, param_name=\"Relative Brightness\", start=0.025, end=.975, step=0.025),\n                                    #\"Cropping\": partial(eval_crop_series, param_name=\"Image size in Pixels\"),\n                                    #\"Contrast\": partial(eval_contrast_series, param_name=\"Relative Contrast\", start=0.1, end=4.5, step=0.1),\n                                    ##\"Resizing\" not supported for batch_mode without a provided truth\n                                    #\"Spike Artefact Intensity\": partial(eval_spike_series, param_name=\"Intensity\", step=.01, start=0, end=.5),\n                                    #\"Spike Artefact Position\": partial(eval_spike_pos_series, param_name=\"X Position of Spike\", step=0.025, intensityFactor=0.2),\n                                    #\"Zoom\": partial(eval_zoom_series, param_name=\"Scale\")\n                                    }, components=['bg','LV','MY','RV'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndihedral_results_without_truths = batch_results(images, model, {\"Orientation\": partial(eval_dihedral_series, param_name=\"Orientation\", mask_prepareSize=False)}, components=['bg','LV','MY','RV'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this demonstration results_with_truths and results_without_truths is the same because in both cases, the truths are predicted by the model, hence we will continue with the demonstration only with one of the two lists."
  },
  {
    "objectID": "batch_mode.html#plotting-the-results",
    "href": "batch_mode.html#plotting-the-results",
    "title": "Batch Mode",
    "section": "Plotting the results",
    "text": "Plotting the results\n\nsource\n\nplot_avg_and_dots\n\n plot_avg_and_dots (df, draw_line=True, dots='single_values',\n                    value_name='Dice Score')\n\nPlots the average dice score and shows the single data points\nPositional arguments: -df (pd.DataFrame object): columns: ‘parameter’, ‘bg’, ‘LV’, ‘MY’, ‘RV’, ‘File’ -draw_line (Boolean): determines if the line between the average of the datapoints is drawn -dots (string): “single_values” or “average”_ determines wether the dots show every single datapoint or one point for the average for each parameter -value_name (str): Name for the Y axis. Depending on which evaluation Score you used in the evaluation function, you can change the Label for the Y axis here.\nReturns: altair.FacetChart object\n\nsource\n\n\nplot_avg_and_errorbars\n\n plot_avg_and_errorbars (df, value_name='Dice Score')\n\nPlots the average dice score and shows the stdev as errorbars.\nPositional arguments: -df (pd.DataFrame object): columns: ‘parameter’, ‘bg’, ‘LV’, ‘MY’, ‘RV’, ‘File’ -value_name (str): Name for the Y axis. Depending on which evaluation Score you used in the evaluation function, you can change the Label for the Y axis here.\nReturns: altair.FacetChart object\n\nsource\n\n\nplot_boxplot\n\n plot_boxplot (df, value_name='Dice Score')\n\nPlots the average dice score as boxplots\nPositional arguments: -df (pd.DataFrame object): columns: ‘parameter’, ‘bg’, ‘LV’, ‘MY’, ‘RV’, ‘File’ -value_name (str): Name for the Y axis. Depending on which evaluation Score you used in the evaluation function, you can change the Label for the Y axis here.\nReturns: altair.FacetChart object\n\nsource\n\n\nplot_batch\n\n plot_batch (df_results, plot_function=<function plot_avg_and_dots>)\n\nCreates and displays the plots with the data as returned by the batch_results functions.\nPositional arguments: -df_results (list): dataframes which contains one dataFrame for each transformation in a format as it is returned by batch_results Keyword arguments: -plot_function: - plot_avg_and_errorbars: plots the average of the dice score of all images across the parameters and shows the standarddeviation as errorbars - plot_avg_and_dots: plots the average of the dice score and additionally shows the single datapoints instead of errorbars - plot_boxplot\nReturns: List with altair.FacetChart objects\n\nplots = plot_batch(results_with_truths, plot_avg_and_errorbars)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the dihedral series it does not make any sense to draw the line between the datapoints, hence the results were stored in a seperate variable to be plotted differently:\n\nplot_batch(dihedral_results_with_truths, partial(plot_avg_and_dots, draw_line=False, dots=\"average\"))\n\n\n\n\n\n\n[alt.FacetChart(...)]\n\n\nWhen calling plot_batch with the parameter partial(plot_avg_and_dots, dots=\"single_values\"), which plots the average plus all the single datapoints, it might look like something went wrong because for some images the dice score seems to be 1 across all parameters. If you look at the images and predictions in detail, like shown below, you can see that for some of the images, the model could not predict any classes, even for the original non-transformed image, which will be used as the truth. Thus, when no prediction can be made for the other transformations the prediction is technically right and results in a dice score of 1. The cause may either be that in the respective images are actually none of the classes present, but the cause can also be, that we are just predicting our truths and it could look different if we used manually created true masks, this has to be kept in mind when interpreting the results.\n\nfor i in images:\n    img = lambda: dicom_to_Image(i)\n    rotation_series = get_rotation_series(img(), model, start=-180, end=180, step=45) #tfm_y=True\n    plot_series(rotation_series, nrow=2, figsize=(25,15), vmax=None, param_name='Degree')"
  },
  {
    "objectID": "myops.html",
    "href": "myops.html",
    "title": "Case Study: MyoPS - myocardial pathology segmentation",
    "section": "",
    "text": "This is a more complex application of misas to a multi-channel input model with multiple output classes. It uses data from the Myocardial pathology segmentation combining multi-sequence CMR challenge (MyoPS 2020).\nGeneral results are published in “Myocardial Pathology Segmentation Combining Multi-Sequence Cardiac Magnetic Resonance Images.” First Challenge, MyoPS 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings and the specific model is described in > Ankenbrand M.J., Lohr D., Schreiber L.M. (2020) “Exploring Ensemble Applications for Multi-sequence Myocardial Pathology Segmentation.” In: Zhuang X., Li L. (eds) Myocardial Pathology Segmentation Combining Multi-Sequence Cardiac Magnetic Resonance Images. MyoPS 2020. Lecture Notes in Computer Science, vol 12554. Springer, Cham.\nsupplemented by https://github.com/chfc-cmi/miccai2020-myops\n\n\n\n\n\n\nWarning\n\n\n\nAs this model uses a specific development version of fastai v2 and data from the challenge can not be freely shared it is much harder to reproduce the results from this notebook. You need to request the data from the challenge website, download the model from zenodo\n\n\n and install the specific versions of packages listed at the bottom of the page (other versions might work too but are untested)."
  },
  {
    "objectID": "myops.html#prepare-model-for-misas",
    "href": "myops.html#prepare-model-for-misas",
    "title": "Case Study: MyoPS - myocardial pathology segmentation",
    "section": "Prepare Model for misas",
    "text": "Prepare Model for misas\n\nfrom fastai.vision.all import *\nfrom fastai.vision.models import resnet34\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nclass AddMaskCodeMapping(Transform):\n    \"Add mapping of pixel value to class for a `TensorMask`\"\n    def __init__(self, mapping, codes=None):\n        #print(\"init\")\n        self.mapping = mapping\n        self.codes = codes\n        if codes is not None: self.vocab,self.c = codes,len(codes)\n\n    def encodes(self, o:PILMask):\n        #print(\"encodes\")\n        mo = ToTensor()(o)\n        mo = mo.to(dtype=torch.long)\n        mo = self.mapping.index_select(0,mo.flatten()).reshape(*mo.shape)\n        mo = PILMask.create(mo.to(dtype=torch.uint8))\n        return mo\n    \n    def decodes(self, o:TensorMask):\n        # decoding of inputs works out of the box, but get_preds are not properly decoded\n        if len(o.shape) > 2:\n            o = o.argmax(dim=0)\n        if self.codes is not None: o._meta = {'codes': self.codes}\n        return o\n\n\ndef MappedMaskBlock(mapping,codes=None):\n    \"A `TransformBlock` for segmentation masks, with mapping of pixel values to classes, potentially with `codes`\"\n    return TransformBlock(type_tfms=PILMask.create, item_tfms=AddMaskCodeMapping(mapping=mapping,codes=codes), batch_tfms=IntToFloatTensor)\n\n\ndef getMappedMaskBlock(predefined_mapping_name):\n    predefined_mappings = {\n        'full': ([0,1,2,3,4,5],['bg','lv','my','rv','ed','sc']),\n        'edOnly': ([0,0,0,0,1,0],['bg','ed']),\n        'edScCombined': ([0,0,0,0,1,1],['bg','edSc']),\n        'scOnly': ([0,0,0,0,0,1],['bg','sc']),\n        'edScOnly': ([0,0,0,0,1,2],['bg','ed','sc']),\n    }\n    mapping,codes = predefined_mappings[predefined_mapping_name]\n    return MappedMaskBlock(mapping = torch.LongTensor(mapping), codes=codes)\n\n\ndef get_train_files(path):\n    items = get_image_files(path)\n    items = L([x for x in items if x.name.startswith(\"1\")])\n    return items\n\n\ndef getMyopsDls(mapping_name=\"full\", images=\"images\", path=\"/storage/biomeds/data/myops/\"):\n    mmb = getMappedMaskBlock(mapping_name)\n    myopsData = DataBlock(blocks=(ImageBlock, mmb),#['bg','lv','my','rv','ed','sc'])),\n        get_items=get_train_files,\n        splitter=FuncSplitter(lambda o: False),\n        get_y=lambda o: str(o).replace(images,\"masks\"),\n        item_tfms=CropPad(256),\n        batch_tfms=aug_transforms(max_rotate=90,pad_mode=\"zeros\"))\n    dls = myopsData.dataloaders(f'{path}/{images}',num_workers=4,batch_size=12)\n    dls[1].bs = 12\n    return dls\n\n\ndef multi_dice(input:Tensor, targs:Tensor, class_id=0, inverse=False)->Tensor:\n    n = targs.shape[0]\n    input = input.argmax(dim=1).view(n,-1)\n    # replace all with class_id with 1 all else with 0 to have binary case\n    output = (input == class_id).float()\n    # same for targs\n    targs = (targs.view(n,-1) == class_id).float()\n    if inverse:\n        output = 1 - output\n        targs = 1 - targs\n    intersect = (output * targs).sum(dim=1).float()\n    union = (output+targs).sum(dim=1).float()\n    res = 2. * intersect / union\n    res[torch.isnan(res)] = 1\n    return res.mean()\n\ndef diceFG(input, targs): return multi_dice(input,targs,class_id=1)\ndef diceLV(input, targs): return multi_dice(input,targs,class_id=1)\ndef diceMY(input, targs): return multi_dice(input,targs,class_id=2)\ndef diceRV(input, targs): return multi_dice(input,targs,class_id=3)\ndef diceEd(input, targs): return multi_dice(input,targs,class_id=4)\ndef diceSc(input, targs): return multi_dice(input,targs,class_id=5)\ndices = [diceLV,diceMY,diceRV,diceEd,diceSc]\n\n\ngetMyopsDls(\"full\", \"images\")\n\n<fastai.data.core.DataLoaders>\n\n\n\nlearn = unet_learner(\n        getMyopsDls(\"full\", \"images\"),\n        resnet34\n    )\n\n[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware."
  },
  {
    "objectID": "myops.html#prepare-dataset-for-misas",
    "href": "myops.html#prepare-dataset-for-misas",
    "title": "Case Study: MyoPS - myocardial pathology segmentation",
    "section": "Prepare Dataset for misas",
    "text": "Prepare Dataset for misas\nData is available as png images and masks which is just fine for misas\n\nfrom misas.core import default_cmap\nfrom PIL import ImageOps\n\n\nimg = lambda: Image.open(\"101-orig-4.png\").convert(\"RGB\")\ntrueMask = lambda: Image.open(\"101-orig-4.png\").convert(\"I\")\nplt.imshow(img())\nplt.imshow(np.array(trueMask()), cmap=default_cmap)\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nclass Fastai2_model:\n    def __init__(self, learner):\n        self.trainedModel = learner\n        \n    def prepareSize(self, item): #, asPIL=False):     \n        to_cut_w=((item.size[0]-256)/2)\n        to_cut_h=((item.size[1]-256)/2)\n        image = ImageOps.crop(item, (np.floor(to_cut_w), np.floor(to_cut_h), np.ceil(to_cut_w), np.ceil(to_cut_h)))\n        return image\n        \n    def predict(self, image):\n        image = self.prepareSize(image)#, #asPIL=True)\n        image = PILImage.create(np.array(image))\n        with self.trainedModel.no_bar():\n            mask = self.trainedModel.predict(image)[0]#(pilimg) #mask,probs,rest \n        output = Image.fromarray(np.array(mask).astype(np.uint8))\n        return output #mask, probs\n\n\nmodel = Fastai2_model(learn.load(\"../../Downloads/multi_ce_full\"))"
  },
  {
    "objectID": "myops.html#how-does-the-trained-model-perform-on-this-training-example",
    "href": "myops.html#how-does-the-trained-model-perform-on-this-training-example",
    "title": "Case Study: MyoPS - myocardial pathology segmentation",
    "section": "How does the trained model perform on this (training) example?",
    "text": "How does the trained model perform on this (training) example?\n#Time to apply the model to the example image and see how it works (we need to call prepareSize manually here):\n\nfrom misas.core import *\nfrom misas.core import default_cmap\n\n\nmodel.prepareSize(img())\n\n\n\n\n\nplt.imshow(np.array(model.prepareSize(trueMask())))\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nmask = model.predict(img())\nplt.imshow(np.array(mask))\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nfig,ax = plt.subplots(figsize=(4.5,4.5))\nplt.imshow(model.prepareSize(img()))\nplt.imshow(mask, cmap=default_cmap)\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nfig,ax = plt.subplots(figsize=(4.5,4.5))\nplt.imshow(model.prepareSize(img()))\nplt.imshow(np.array(model.prepareSize((trueMask()))), cmap=default_cmap)\n\n<matplotlib.image.AxesImage>"
  },
  {
    "objectID": "myops.html#robustness-to-basic-transformations",
    "href": "myops.html#robustness-to-basic-transformations",
    "title": "Case Study: MyoPS - myocardial pathology segmentation",
    "section": "Robustness to basic transformations",
    "text": "Robustness to basic transformations\n\n#img = lambda: open_image(files[0]).resize(256)\n#trueMask = lambda: open_mask(files[0].replace(\"image\",\"mask\"))\n\n\nSensitivity to orientation\nChanges in orientation are very common. Not because it is common to acquire images in different orientation but because the way data is stored in different file formats like nifti and dicom differs. So it is interesting to see how the model works in all possible orientations (including flips).\n\ndihed = get_dihedral_series(img(),model)\nplot_series(dihed, nrow=2, figsize=(20,12))\n\n\n\n\n\n\n\n\n#plt.imshow(np.array(dihed [0][2].convert (\"I\")))\n#dihed[0][1]\n\n\n\nSensitivity to rotation\nLet’s get an impression of how quickly segmentation performance decreases with deviations in rotation.\n\nplot_series(get_rotation_series(img(),model, step=30), nrow=2, param_name=\"deg\")\n\n\n\n\n\n\n\n\nplot_series(get_rotation_series(img(),model, step=60), nrow=1, param_name=\"deg\")\n\n\n\n\n\n\n\n\nresults = eval_rotation_series(img(),trueMask(),model,start=-180,end=180,components=[\"bg\",\"LV\",\"MY\",\"RV\",\"edema\",\"scar\"])\n\n\n\n\n\nimport altair as alt\n\n\n(alt\n .Chart(results.melt(id_vars=['deg'],value_vars=['LV','MY','RV','edema','scar']))\n .mark_line()\n .encode(\n     x=\"deg\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\nSo the range where prediction performance remains stable is quite large for most classes. However the rarer pathology classes scar and particularly edema react much more sensitively to rotation.\n\ngif_series(\n    get_rotation_series(img(),model, start=0, end=360,step=10),\n    \"example/myops/rotation.gif\",\n    param_name=\"deg\",\n    duration=400\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to rotation\n\n\n\n\nSensitivity to cropping\nAnother variation that might occur in real life is a difference in field of view. This can happen due to different settings when acquiring the images or due to pre-processing steps in an analysis pipeline.\n\nplot_series(get_crop_series(img(),model, start = 50, end = 230, step = 10, finalSize=400), nrow=2, vmax=5)\n\n\n\n\n\n\n\n\nplot_series(get_crop_series(img(),model, start = 80, end = 230, step = 20, finalSize=400), nrow=1, vmax=5)\n\n\n\n\n\n\n\n\ngif_series(\n    get_crop_series(img(),model, start=50, end=250,step=10),\n    \"example/myops/crop.gif\",\n    param_name=\"pixels\",\n    duration=400,\n    vmax=5\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to rotation - myops\n\n\nIt seems to be okay to crop the image to some extend. But performance degrades even before we start to crop part of the heart.\n\nresults = eval_crop_series(img(),trueMask(),model,start = 50, end=256, finalSize=400, components=[\"bg\",\"LV\",\"MY\",\"RV\",\"edema\",\"scar\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['pixels'],value_vars=['LV','MY','RV','edema','scar']))\n .mark_line()\n .encode(\n     x=\"pixels\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\n\n\nSensitivity to brightness\n\nplot_series(get_brightness_series(img(),model), nrow=1) #end = 0.99, step = 0.18\n\n\n\n\n\n\n\n\nresults = eval_bright_series(img(),trueMask(),model, components=[\"bg\",\"LV\",\"MY\",\"RV\",\"edema\",\"scar\"])\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['brightness'],value_vars=['LV','MY','RV','edema','scar']))\n .mark_line()\n .encode(\n     x=\"brightness\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\n\n\nSensitivity to contrast\n\nplot_series(get_contrast_series(img(),model), nrow = 1, vmax=5) #start=0.1, end=3, step=0.5\n\n\n\n\n\n\n\n\nresults = eval_contrast_series(img(),trueMask(),model, components=[\"bg\",\"LV\",\"MY\",\"RV\",\"edema\",\"scar\"]) #end = 2.5, step = 0.3\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['contrast'],value_vars=['LV','MY','RV','edema','scar']))\n .mark_line()\n .encode(\n     x=\"contrast\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\n\n\nSensitivity to zoom\n\nplot_series(get_zoom_series(img(),model), param_name=\"zoom\", nrow=2, vmax=5) #start=160,end=750,step=60, finalSize=480)\n\n\n\n\n\n\n\n\nplot_series(get_zoom_series(img(),model), param_name=\"zoom\", nrow=1, vmax=5)#,start=160,end=770,step=120, finalSize=480)\n\n\n\n\n\n\n\n\nresults = eval_zoom_series(img(),trueMask(),model,components=[\"bg\",\"LV\",\"MY\",\"RV\",\"edema\",\"scar\"]) #,start=160,end=900,step=20,finalSize=480\n\n\n\n\n\n(alt\n .Chart(results.melt(id_vars=['scale'],value_vars=['LV','MY','RV','edema','scar']))\n .mark_line()\n .encode(\n     x=\"scale\",\n     y=\"value\",\n     color=\"variable\",\n     tooltip=\"value\"\n )\n .properties(width=700,height=300)\n .interactive()\n)\n\n\n\n\n\n\n\ngif_series(\n    get_zoom_series(img(),model) , #start=50, end=900,step=50\n    \"example/myops/zoom.gif\",\n    param_name=\"scale\",\n    duration=400,\n    vmax=5\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to zoom\n\n\nIDEA: Channel imbalance transformation For multi-channel images it might be useful to consider transformations that work differently on different channels."
  },
  {
    "objectID": "myops.html#robustness-to-mr-artifacts",
    "href": "myops.html#robustness-to-mr-artifacts",
    "title": "Case Study: MyoPS - myocardial pathology segmentation",
    "section": "Robustness to MR artifacts",
    "text": "Robustness to MR artifacts\nIt would be nice to analyze the effect of MR artifacts. However, we are dealing with multi-channel images here. Each channel is a separate MR image. So it is not obvious how to deal with this."
  },
  {
    "objectID": "myops.html#package-versions",
    "href": "myops.html#package-versions",
    "title": "Case Study: MyoPS - myocardial pathology segmentation",
    "section": "Package Versions",
    "text": "Package Versions\n# packages in environment at /opt/conda/envs/misas:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       1_gnu    conda-forge\nabsl-py                   0.11.0                   pypi_0    pypi\naltair                    4.1.0                    pypi_0    pypi\nargon2-cffi               20.1.0                   pypi_0    pypi\nastunparse                1.6.3                    pypi_0    pypi\nasync-generator           1.10                     pypi_0    pypi\nattrs                     20.3.0                   pypi_0    pypi\nbackcall                  0.2.0                    pypi_0    pypi\nbeautifulsoup4            4.9.3                    pypi_0    pypi\nbleach                    3.2.1                    pypi_0    pypi\nblis                      0.7.4                    pypi_0    pypi\nbottleneck                1.3.2                    pypi_0    pypi\nca-certificates           2020.12.5            ha878542_0    conda-forge\ncachetools                4.2.0                    pypi_0    pypi\ncatalogue                 1.0.0                    pypi_0    pypi\ncertifi                   2020.12.5        py36h5fab9bb_0    conda-forge\ncffi                      1.14.4                   pypi_0    pypi\nchardet                   4.0.0                    pypi_0    pypi\nclick                     7.1.2                    pypi_0    pypi\ncontextvars               2.4                      pypi_0    pypi\ncycler                    0.10.0                   pypi_0    pypi\ncymem                     2.0.5                    pypi_0    pypi\ndataclasses               0.8                      pypi_0    pypi\ndecorator                 4.4.2                    pypi_0    pypi\ndefusedxml                0.6.0                    pypi_0    pypi\nentrypoints               0.3                      pypi_0    pypi\nfastai                    1.0.61                   pypi_0    pypi\nfastai2                   0.0.30                   pypi_0    pypi\nfastcore                  1.0.0                    pypi_0    pypi\nfastinference             0.0.35                   pypi_0    pypi\nfastprogress              1.0.0                    pypi_0    pypi\nfastscript                1.0.0                    pypi_0    pypi\nflatbuffers               1.12                     pypi_0    pypi\nfuture                    0.18.2                   pypi_0    pypi\ngast                      0.3.3                    pypi_0    pypi\ngif                       2.0.0                    pypi_0    pypi\ngoogle-auth               1.24.0                   pypi_0    pypi\ngoogle-auth-oauthlib      0.4.2                    pypi_0    pypi\ngoogle-pasta              0.2.0                    pypi_0    pypi\ngrpcio                    1.32.0                   pypi_0    pypi\nh5py                      2.10.0                   pypi_0    pypi\nhumanize                  3.2.0                    pypi_0    pypi\nidna                      2.10                     pypi_0    pypi\nimageio                   2.9.0                    pypi_0    pypi\nimmutables                0.14                     pypi_0    pypi\nimportlib-metadata        3.3.0                    pypi_0    pypi\nipykernel                 5.4.2                    pypi_0    pypi\nipython                   7.16.1                   pypi_0    pypi\nipython-genutils          0.2.0                    pypi_0    pypi\nipywidgets                7.6.2                    pypi_0    pypi\njedi                      0.18.0                   pypi_0    pypi\njinja2                    2.11.2                   pypi_0    pypi\njoblib                    1.0.0                    pypi_0    pypi\njsonschema                3.2.0                    pypi_0    pypi\njupyter                   1.0.0                    pypi_0    pypi\njupyter-client            6.1.7                    pypi_0    pypi\njupyter-console           6.2.0                    pypi_0    pypi\njupyter-core              4.7.0                    pypi_0    pypi\njupyterlab-pygments       0.1.2                    pypi_0    pypi\njupyterlab-widgets        1.0.0                    pypi_0    pypi\nkeras-preprocessing       1.1.2                    pypi_0    pypi\nkiwisolver                1.3.1                    pypi_0    pypi\nkornia                    0.2.2                    pypi_0    pypi\nlibgcc-ng                 9.3.0               h5dbcf3e_17    conda-forge\nlibgomp                   9.3.0               h5dbcf3e_17    conda-forge\nmarkdown                  3.3.3                    pypi_0    pypi\nmarkupsafe                1.1.1                    pypi_0    pypi\nmatplotlib                3.3.3                    pypi_0    pypi\nmisas                     0.0.3                     dev_0    <develop>\nmistune                   0.8.4                    pypi_0    pypi\nmurmurhash                1.0.5                    pypi_0    pypi\nnbclient                  0.5.1                    pypi_0    pypi\nnbconvert                 6.0.7                    pypi_0    pypi\nnbdev                     1.0.10                   pypi_0    pypi\nnbformat                  5.0.8                    pypi_0    pypi\nncurses                   5.9                          10    conda-forge\nnest-asyncio              1.4.3                    pypi_0    pypi\nnetworkx                  2.5                      pypi_0    pypi\nnibabel                   3.2.1                    pypi_0    pypi\nnotebook                  6.1.6                    pypi_0    pypi\nnumexpr                   2.7.2                    pypi_0    pypi\nnumpy                     1.19.4                   pypi_0    pypi\nnvidia-ml-py3             7.352.0                  pypi_0    pypi\noauthlib                  3.1.0                    pypi_0    pypi\nonnxruntime               1.6.0                    pypi_0    pypi\nopenssl                   1.0.2u               h516909a_0    conda-forge\nopt-einsum                3.3.0                    pypi_0    pypi\npackaging                 20.8                     pypi_0    pypi\npandas                    0.25.3                   pypi_0    pypi\npandocfilters             1.4.3                    pypi_0    pypi\nparso                     0.8.1                    pypi_0    pypi\npexpect                   4.8.0                    pypi_0    pypi\npickleshare               0.7.5                    pypi_0    pypi\npillow                    8.1.0                    pypi_0    pypi\npip                       20.3.3             pyhd8ed1ab_0    conda-forge\nplac                      1.1.3                    pypi_0    pypi\npreshed                   3.0.5                    pypi_0    pypi\nprometheus-client         0.9.0                    pypi_0    pypi\nprompt-toolkit            3.0.3                    pypi_0    pypi\nprotobuf                  3.14.0                   pypi_0    pypi\nptyprocess                0.7.0                    pypi_0    pypi\npyasn1                    0.4.8                    pypi_0    pypi\npyasn1-modules            0.2.8                    pypi_0    pypi\npycparser                 2.20                     pypi_0    pypi\npydicom                   2.1.1                    pypi_0    pypi\npygments                  2.7.3                    pypi_0    pypi\npyparsing                 2.4.7                    pypi_0    pypi\npyrsistent                0.17.3                   pypi_0    pypi\npython                    3.6.5                         1    conda-forge\npython-dateutil           2.8.1                    pypi_0    pypi\npython_abi                3.6                     1_cp36m    conda-forge\npytz                      2020.5                   pypi_0    pypi\npywavelets                1.1.1                    pypi_0    pypi\npyyaml                    5.3.1                    pypi_0    pypi\npyzmq                     20.0.0                   pypi_0    pypi\nqtconsole                 5.0.1                    pypi_0    pypi\nqtpy                      1.9.0                    pypi_0    pypi\nreadline                  7.0                           0    conda-forge\nrequests                  2.25.1                   pypi_0    pypi\nrequests-oauthlib         1.3.0                    pypi_0    pypi\nrsa                       4.6                      pypi_0    pypi\nscikit-image              0.17.2                   pypi_0    pypi\nscikit-learn              0.24.0                   pypi_0    pypi\nscipy                     1.5.4                    pypi_0    pypi\nsend2trash                1.5.0                    pypi_0    pypi\nsetuptools                49.6.0           py36h9880bd3_2    conda-forge\nsimpleitk                 1.2.4                    pypi_0    pypi\nsix                       1.15.0                   pypi_0    pypi\nsoupsieve                 2.1                      pypi_0    pypi\nspacy                     2.3.5                    pypi_0    pypi\nsqlite                    3.20.1                        2    conda-forge\nsrsly                     1.0.5                    pypi_0    pypi\ntensorboard               2.4.0                    pypi_0    pypi\ntensorboard-plugin-wit    1.7.0                    pypi_0    pypi\ntensorflow                2.4.0                    pypi_0    pypi\ntensorflow-estimator      2.4.0                    pypi_0    pypi\ntermcolor                 1.1.0                    pypi_0    pypi\nterminado                 0.9.2                    pypi_0    pypi\ntestpath                  0.4.4                    pypi_0    pypi\nthinc                     7.4.5                    pypi_0    pypi\nthreadpoolctl             2.1.0                    pypi_0    pypi\ntifffile                  2020.9.3                 pypi_0    pypi\ntk                        8.6.10               h21135ba_1    conda-forge\ntoolz                     0.11.1                   pypi_0    pypi\ntorch                     1.6.0                    pypi_0    pypi\ntorchio                   0.18.15                  pypi_0    pypi\ntorchvision               0.8.2                    pypi_0    pypi\ntornado                   6.1                      pypi_0    pypi\ntqdm                      4.55.1                   pypi_0    pypi\ntraitlets                 4.3.3                    pypi_0    pypi\ntyping-extensions         3.7.4.3                  pypi_0    pypi\nurllib3                   1.26.2                   pypi_0    pypi\nwasabi                    0.8.0                    pypi_0    pypi\nwcwidth                   0.2.5                    pypi_0    pypi\nwebencodings              0.5.1                    pypi_0    pypi\nwerkzeug                  1.0.1                    pypi_0    pypi\nwheel                     0.36.2             pyhd3deb0d_0    conda-forge\nwidgetsnbextension        3.5.1                    pypi_0    pypi\nwrapt                     1.12.1                   pypi_0    pypi\nxz                        5.2.5                h516909a_1    conda-forge\nzipp                      3.4.0                    pypi_0    pypi\nzlib                      1.2.11            h516909a_1010    conda-forge"
  },
  {
    "objectID": "fastai_model.html",
    "href": "fastai_model.html",
    "title": "Fastai Model (misas)",
    "section": "",
    "text": "source\n\nFastai2_model\n\n Fastai2_model (github, model, force_reload=False)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "tensorflow_model.html",
    "href": "tensorflow_model.html",
    "title": "Tensorflow Model (misas)",
    "section": "",
    "text": "source\n\ncrop_pad_pil\n\n crop_pad_pil (image, size)\n\n\nsource\n\n\nukbb_model\n\n ukbb_model (model_path)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "local_interpret.html",
    "href": "local_interpret.html",
    "title": "Local Interpretability",
    "section": "",
    "text": "from misas.fastai_model import Fastai2_model"
  },
  {
    "objectID": "local_interpret.html#evaluation-functions",
    "href": "local_interpret.html#evaluation-functions",
    "title": "Local Interpretability",
    "section": "Evaluation functions",
    "text": "Evaluation functions"
  },
  {
    "objectID": "local_interpret.html#sensitivity-analysis",
    "href": "local_interpret.html#sensitivity-analysis",
    "title": "Local Interpretability",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nExample data (kaggle)\n\ndef label_func(x):\n    pass\ndef acc_seg(input, target):\n    pass\ndef diceComb(input, targs):\n    pass\ndef diceLV(input, targs):\n    pass\ndef diceMY(input, targs):\n    pass\n\n\ndef img():\n    \"\"\"\n    Opens the sample image as a PIL image\n    \"\"\"\n    return Image.open(\"example/kaggle/images/1-frame014-slice005.png\").convert(\"RGB\")\n#img = lambda: Image.open(\"example/kaggle/images/1-frame014-slice005.png\")\n\ndef trueMask():\n    \"\"\"\n    Opens the true mask as a PIL image\n    \"\"\"\n    return Image.open(\"example/kaggle/masks/1-frame014-slice005.png\").convert(\"I\")\n#trueMask = lambda: Image.open(\"example/kaggle/masks/1-frame014-slice005.png\")\ntrainedModel = Fastai2_model(\"chfc-cmi/cmr-seg-tl\", \"cmr_seg_base\")\n\nUsing cache found in /home/csa84mikl/.cache/torch/hub/chfc-cmi_cmr-seg-tl_master\n\n\n\n\nDefault color map\nDefine a default color map for the sample image derived from viridis and a default color map for the true map derived from plasma but setting the color for class “0” to completely transparent. This makes sense if class “0” is the background.\n\n\nGeneric functions\n\nsource\n\n\nget_generic_series\n\n get_generic_series (image, model, transform, truth=None, tfm_y=False,\n                     start=0, end=180, step=30, log_steps=False)\n\nGeneric function for transforming images. Input: image (PIP image, usually your sample image opened by img()), model (the function for your model that manages the prediction for your mask), transform (your specific transformation function), truth = None (replaces with a true mask if available), tfm_y = False (set to True if your true mask has to be transformed as well to fit the transformed sample image e.g in case of a rotation of the sample image), start, end, step as values the transform function, log_steps = False (if enabled logarithmic steps as parameters for the transform function are possible) Output: a list containing lists of [param, img, pred, trueMask] after img and optionally trueMask have been transformed and pred has been determined by using a modell on the transformed img for each different param\n\nsource\n\n\nplot_series\n\n plot_series (series, nrow=1, figsize=(16, 6), param_name='param',\n              overlay_truth=False, vmax=None, vmin=0,\n              cmap=<matplotlib.colors.ListedColormap object at\n              0x7f52ec9228e0>,\n              cmap_true_mask=<matplotlib.colors.ListedColormap object at\n              0x7f520556ddf0>, **kwargs)\n\nplots the transformed images with the prediction and optionally the true mask overlayed intput: series = a list containing lists of [param, img, pred, trueMask] from the function get_generic_series nrow = number of rows drawn with the transformed images figsize = (16,6) param_name=‘param’, overlay_truth = False (if True displays the true mask over the sample along with the prediction) vmax = None (controls how many colors the prediction is going to have, can be set manually by the user, otherwise is deterimed by the max amount of colors in the prediction cmap= default_cmap (sets the default color map) output: a plot generated by mathplotlib\n\nsource\n\n\nplot_frame\n\n plot_frame (param, img, pred, param_name='param', vmax=None, vmin=0,\n             cmap=<matplotlib.colors.ListedColormap object at\n             0x7f52ec9228e0>, **kwargs)\n\nplots the transformed images and prediction overlayed for the gif_series function\n\nsource\n\n\ngif_series\n\n gif_series (series, fname, duration=150, param_name='param', vmax=None,\n             vmin=0, cmap=<matplotlib.colors.ListedColormap object at\n             0x7f52ec9228e0>)\n\ncreates a gif from the output of plot_frame\n\nsource\n\n\neval_generic_series\n\n eval_generic_series (image, mask, model, transform_function, start=0,\n                      end=360, step=5, param_name='param',\n                      mask_transform_function=None, components=['bg',\n                      'c1', 'c2'], eval_function=<function\n                      dice_by_component>, mask_prepareSize=True)\n\nPerform the transformation on the sample, creates a prediction and then uses the prediction and true mask to run an evaluation function to measure the overlap between predicted mask and true mask\n\nsource\n\n\nplot_eval_series\n\n plot_eval_series (results, chart_type='line', value_vars=None,\n                   value_name='Dice Score')\n\nPlots the resuls of the eval_generic_function\n\n\nRotation\n\nsource\n\n\nget_rotation_series\n\n get_rotation_series (image, model, start=0, end=361, step=60, **kwargs)\n\nruns the get_generic_series with rotationTransform as transform\n\nsource\n\n\nrotationTransform\n\n rotationTransform (image, deg)\n\nrotates an image by x degrees (deg)\n\nseries = get_rotation_series(img(), trainedModel, truth=trueMask())\n\n\n\n\n[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\nplot_series(series, overlay_truth = True)\n\n\n\n\n\nsource\n\n\neval_rotation_series\n\n eval_rotation_series (image, mask, model, step=5, start=0, end=360,\n                       param_name='deg', **kwargs)\n\n\nresults = eval_rotation_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'])\nplot_eval_series(results)\n\n\n\n\n\n\n\n\n\nYou can easily generate gifs by plotting multiple frames\n\ngif_series(\n    get_rotation_series(img(),trainedModel,start=0,end=360,step=5),\n    \"example/kaggle/rotation.gif\",\n    duration=500,\n    param_name=\"degrees\"\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to rotation\n\n\n\n\nCropping\n\nsource\n\n\nget_crop_series\n\n get_crop_series (image, model, start=0, end=256, step=10, finalSize=None,\n                  **kwargs)\n\n\nsource\n\n\ncropTransform\n\n cropTransform (image, pxls, finalSize=None)\n\n\nseries = get_crop_series(img(), trainedModel, truth=trueMask(), step=10)\n\n\n\n\n\nplot_series(series,nrow=3,figsize=(16,15), overlay_truth = True) #,overlay_truth=True)\n\n\n\n\n\nsource\n\n\neval_crop_series\n\n eval_crop_series (image, mask, model, step=10, start=0, end=256,\n                   finalSize=None, param_name='pixels', **kwargs)\n\n\nresults = eval_crop_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'])\nplot_eval_series(results)\n\n\n\n\n\n\n\n\n\nCropping and comparing to the full original mask might not be desired. In this case it is possible to crop the mask as well. All pixels in the cropped area are set to 0 (commonly the background class). As soon as a class is completely missing, the dice score might jump to 1 because not predicting the class is correct in that case.\n\ngif_series(\n    get_crop_series(img(),trainedModel,start=0,end=256,step=5),\n    \"example/kaggle/crop.gif\",\n    duration=500,\n    param_name=\"pixels\"\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to cropping\n\n\n\n\nBrightness\n\nsource\n\n\nget_brightness_series\n\n get_brightness_series (image, model, start=0.25, end=8,\n                        step=1.4142135623730951, log_steps=True, **kwargs)\n\n\nsource\n\n\nbrightnessTransform\n\n brightnessTransform (image, light)\n\n\nseries = get_brightness_series(img(), trainedModel, truth=trueMask(), start=1/8, end=16)\n\n\n\n\n\nplot_series(series, nrow=3, figsize=(12,6), overlay_truth = True)\n\n\n\n\n\nsource\n\n\neval_bright_series\n\n eval_bright_series (image, mask, model, start=0.05, end=0.95, step=0.05,\n                     param_name='brightness', **kwargs)\n\n\nresults = eval_bright_series(img(), trueMask(), trainedModel, start=0, end=1.05, components=['bg','LV','MY'])\nplot_eval_series(results)\n\n\n\n\n\n\n\n\n\n\ngif_series(\n    get_brightness_series(img(), trainedModel, step = np.sqrt(2)),\n    \"example/kaggle/brightness.gif\",\n    duration=500,\n    param_name=\"brightness\"\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to brightness\n\n\n\n\nContrast\n\nsource\n\n\nget_contrast_series\n\n get_contrast_series (image, model, start=0.25, end=8,\n                      step=1.4142135623730951, log_steps=True, **kwargs)\n\n\nsource\n\n\ncontrastTransform\n\n contrastTransform (image, scale)\n\n\nseries = get_contrast_series(img(), trainedModel, truth=trueMask(), start=1/8, end=16)\n\n\n\n\n\nplot_series(series, nrow=3, figsize=(12,8), overlay_truth = True)\n\n\n\n\n\nsource\n\n\neval_contrast_series\n\n eval_contrast_series (image, mask, model, start=0.25, end=8,\n                       step=1.4142135623730951, param_name='contrast',\n                       **kwargs)\n\n\nresults = eval_contrast_series(img(), trueMask(), trainedModel, start=0.25, end=8, step=np.sqrt(2), components=['bg','LV','MY'])\nplot_eval_series(results)\n\n\n\n\n\n\n\n\n\n\ngif_series(\n    get_contrast_series(img(), trainedModel,step=np.sqrt(2)),\n    \"example/kaggle/contrast.gif\",\n    duration=500,\n    param_name=\"contrast\"\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to contrast\n\n\n\n\nZoom\n\nsource\n\n\nget_zoom_series\n\n get_zoom_series (image, model, start=0, end=1, step=0.1, finalSize=None,\n                  **kwargs)\n\n\nsource\n\n\nzoomTransform\n\n zoomTransform (image, zoom, finalSize=None)\n\n\nseries = get_zoom_series(img(), trainedModel, truth=trueMask())\n\n\n\n\n\nplot_series(series, nrow=2, figsize=(16,8), overlay_truth = True)\n\n\n\n\n\nsource\n\n\neval_zoom_series\n\n eval_zoom_series (image, mask, model, step=0.1, start=0, end=1,\n                   finalSize=None, param_name='scale', **kwargs)\n\n\nresults = eval_zoom_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'])\nplot_eval_series(results)\n\n\n\n\n\n\n\n\n\n\ngif_series(\n    get_zoom_series(img(),trainedModel,start=0,end=1,step=0.1),\n    \"example/kaggle/zoom.gif\",\n    duration=500,\n    param_name=\"scale\"\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to zoom\n\n\n\n\nDihedral\n\nsource\n\n\nget_dihedral_series\n\n get_dihedral_series (image, model, start=0, end=8, step=1, **kwargs)\n\n\nsource\n\n\ndihedralTransform\n\n dihedralTransform (image, sym_im)\n\n\nseries = get_dihedral_series(img(), trainedModel, truth=trueMask())\n\n\n\n\n\nplot_series(series, overlay_truth = True)\n\n\n\n\n\nsource\n\n\neval_dihedral_series\n\n eval_dihedral_series (image, mask, model, start=0, end=8, step=1,\n                       param_name='k', **kwargs)\n\n\nresults = eval_dihedral_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'])\nplot_eval_series(results, chart_type=\"point\")\n\n\n\n\n\n\n\n\n\n\ngif_series(\n    get_dihedral_series(img(), trainedModel),\n    \"example/kaggle/dihedral.gif\",\n    param_name=\"k\",\n    duration=1000\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to dihedral\n\n\n\n\nResize\n\nsource\n\n\nresizeTransform\n\n resizeTransform (image, size)\n\n\nsource\n\n\nget_resize_series\n\n get_resize_series (image, model, start=10, end=200, step=30, **kwargs)\n\n\nseries = get_resize_series(img(), trainedModel, truth=trueMask())\n\n\n\n\n\nplot_series(series, sharex=True, sharey=True, overlay_truth = True)\n\n\n\n\n\nsource\n\n\neval_resize_series\n\n eval_resize_series (image, mask, model, start=22, end=3000, step=100,\n                     param_name='px', **kwargs)\n\n\nresults = eval_resize_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'])\nplot_eval_series(results)\n\n\n\n\n\n\n\n\n\n\ngif_series(\n    get_resize_series(img(), trainedModel),\n    \"example/kaggle/resize.gif\",\n    param_name=\"px\",\n    duration=500\n)\n\n\n\n\n\n\n\nsegmentation sensitivity to resizing"
  },
  {
    "objectID": "local_interpret.html#more-on-evaluation",
    "href": "local_interpret.html#more-on-evaluation",
    "title": "Local Interpretability",
    "section": "More on Evaluation",
    "text": "More on Evaluation\nThe default score for evaluation is the Dice-Score calculated separately for each component. In addition to Dice, misas provides functions for component-wise functions for precision and recall but you can easily define your own.\n\nresults_dice = eval_rotation_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'])\nplot_dice = plot_eval_series(results_dice, value_vars=['bg','LV','MY'], value_name=\"Dice Score\")\n\n\n\n\n\nresults_precision = eval_rotation_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'], eval_function=precision_by_component)\nplot_precision = plot_eval_series(results_precision, value_vars=['bg','LV','MY'], value_name=\"Precision\")\n\n\n\n\n\nresults_recall = eval_rotation_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'], eval_function=recall_by_component)\nplot_recall = plot_eval_series(results_recall, value_vars=['bg','LV','MY'], value_name=\"Recall\")\n\n\n\n\nThe objects returned by the plot function are altair graphs that can be further customized and combined\n\nplot_dice = plot_dice.properties(title=\"Dice\")\nplot_precision = plot_precision.properties(title=\"Precision\")\nplot_recall = plot_recall.properties(title=\"Recall\")\n\n\nplot_dice & plot_precision & plot_recall\n\n\n\n\n\n\nIn order to define your own evaluation function you need to define a function with the predicted and true masks as first and second parameter and the component to evaluate as third parameter. Masks are of type ImageSegment and you can access the tensor data using the .data property. This is an example on how to define specificity. This can than be passed as evaluation function.\n\ndef specificity_by_component(predictedMask, trueMask, component = 1):\n    specificity = 1.0\n    pred = np.array(predictedMask) != component\n    msk = np.array(trueMask) != component\n    intersect = pred&msk\n    total = np.sum(pred) + np.sum(msk)\n    if total > 0:\n        specificity = np.sum(intersect).astype(float) / msk.sum()\n    return specificity.item()\n\n\nresults_specificity = eval_rotation_series(img(), trueMask(), trainedModel, components=['bg','LV','MY'], eval_function=specificity_by_component)\nplot_specificity = plot_eval_series(results_specificity, value_vars=['bg','LV','MY'], value_name=\"Specificity\")\nplot_specificity\n\n\n\n\n\n\n\n\n\nThe specificity for the background class degrades so dramatically for rotations of around 180 degrees as the LV and MY classes are no longer detected at all. So there are no “true negatives” for the background class, consequently specificity for that class drops to zero."
  },
  {
    "objectID": "local_interpret.html#confusion-matrices",
    "href": "local_interpret.html#confusion-matrices",
    "title": "Local Interpretability",
    "section": "Confusion Matrices",
    "text": "Confusion Matrices\nConfusion matrices are useful to evaluate in more detail which classes the model gets wrong. To conveniently generate separate confusion matrices or series misas provides some convenience functions.\n\nseries = get_rotation_series(img(), trainedModel, truth=trueMask())\n\n\n\n\n\nsource\n\nget_confusion\n\n get_confusion (prediction, truth, max_class=None)\n\nThe get_confusion function returns a two dimensional numpy array with counts for each class combination. The true class is along the columns and the predicted class along the rows. The number of classes is derived from the data if not provided as max_class parameter. This parameter is important if the given instance of prediction and truth does not contain all available classes.\n\ncm = get_confusion(series[0][2], series[0][3])\ncm\n\narray([[63862,    22,    37],\n       [    8,   756,    76],\n       [  129,    28,   618]])\n\n\nThis matrix shows that there are 754 pixels classified correctly as “LV” (class=1). However, there are also 17 pixels that are in reality “LV” but predicted as “MY”. Accordingly, there are 68 pixels that are “MY” but predicted as “LV”.\nLooking at tables is much less convenient and informative than looking at graphics so let’s plot this matrix\n\nsource\n\n\nplot_confusion\n\n plot_confusion (confusion_matrix, norm_axis=0, components=None, ax=None,\n                 ax_label=True, cmap='Blues')\n\n\n_ = plot_confusion(cm, components=[\"bg\",\"LV\",\"MY\"])\n\n\n\n\nThis is the confusion matrix for one image. Next we want to look at the confusion matrix for a full series of transformed (in this case rotated) images.\n\nsource\n\n\nplot_confusion_series\n\n plot_confusion_series (series, nrow=1, figsize=(16, 6),\n                        param_name='param', cmap='Blues', components=None,\n                        norm_axis=0, **kwargs)\n\n\nplot_series(series,figsize=(16.5,6))\n\n\n\n\n\nplot_confusion_series(series, components=['bg','LV','MY'])"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "misas",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "",
    "text": "Input alterations currently include: - rotation - cropping - brightness - contrast - zooming - flipping (dihedral) - resizing - MR artifacts (via torchio)"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "Install",
    "text": "Install\npip install misas"
  },
  {
    "objectID": "index.html#cite",
    "href": "index.html#cite",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "Cite",
    "text": "Cite\nIf you use misas in your research, please cite: > Ankenbrand, M. J., Shainberg, L., Hock, M., Lohr, D., & Schreiber, L. M. Sensitivity analysis for interpretation of machine learning based segmentation models in cardiac MRI. BMC Medical Imaging, 21(27). https://doi.org/10.1186/s12880-021-00551-1\nIf you use the simulated MR artifacts, please also cite torchio: > F. Pérez-García, R. Sparks, and S. Ourselin. TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. Computer Methods and Programs in Biomedicine (June 2021), p. 106236. ISSN: 0169-2607. https://doi.org/10.1016/j.cmpb.2021.106236"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "How to use",
    "text": "How to use\nExample with kaggle data\n\nfrom misas.core import *\nfrom misas.core import default_cmap, default_cmap_true_mask\nfrom misas.fastai_model import Fastai2_model\nfrom PIL import Image, ImageEnhance, ImageOps\nfrom functools import partial\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef label_func(x):\n    pass\ndef acc_seg(input, target):\n    pass\ndef diceComb(input, targs):\n    pass\ndef diceLV(input, targs):\n    pass\ndef diceMY(input, targs):\n    pass\n\n\nimg = lambda: Image.open(\"example/kaggle/images/1-frame014-slice005.png\").convert(\"RGB\")\ntrueMask = lambda: Image.open(\"example/kaggle/masks/1-frame014-slice005.png\").convert(\"I\")\ntrainedModel = Fastai2_model(\"chfc-cmi/cmr-seg-tl\", \"cmr_seg_base\", force_reload=False)\nfig, ax = plt.subplots(figsize=(8,8))\nax.imshow(np.array(img()))\nax.imshow(np.array(trueMask()), cmap=default_cmap_true_mask, alpha=.5, interpolation=\"nearest\")\nax.axes.xaxis.set_visible(False)\nax.axes.yaxis.set_visible(False)\n\nUsing cache found in /home/csa84mikl/.cache/torch/hub/chfc-cmi_cmr-seg-tl_master\n\n\n\n\n\n\nRotation\n\nplot_series(get_rotation_series(img(), trainedModel))\n\n\n\n\n[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n\n\n\n\n\n\nresults = eval_rotation_series(img(), trueMask(), trainedModel)\nplt.plot(results['deg'], results['c1'])\nplt.plot(results['deg'], results['c2'])\nplt.axis([0,360,0,1])\n\n\n\n\n(0.0, 360.0, 0.0, 1.0)\n\n\n\n\n\nYou can use interactive elements to manually explore the impact of rotation\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n\nrotation_series = get_rotation_series(img(),trainedModel,step=10)\n\n\n\n\n\ndef plot_rotation_frame(deg):\n    return plot_frame(*rotation_series[int(deg/10)], figsize=(10,10))\n\n\ninteract(\n    plot_rotation_frame,\n    deg=widgets.IntSlider(min=0, max=360, step=10, value=90, continuous_update=False)\n)\n\n\n\n\n<function __main__.plot_rotation_frame(deg)>\n\n\nThere are lots of other transformations to try (e.g. cropping, brightness, contrast, …) as well as MR specific artifacts."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "Overview",
    "text": "Overview\nThis is the schematic overview of how misas works. Created with the amazing Excalidraw."
  },
  {
    "objectID": "index.html#logo",
    "href": "index.html#logo",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "Logo",
    "text": "Logo\nThe logo was designed by Markus J. Ankenbrand using: - Open box / Boite ouverte by SimpleIcons via openclipart.org - Cutter icon by SimpleIcons via openclipart.org, original by Marco Olgio, via WikiMedia - Hack Font - Inkscape"
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "Attribution",
    "text": "Attribution\nThis project is inspired by the awesome “Is it a Duck or Rabbit” tweet by @minimaxir. Also check out the corresponding repo.\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Is it a Duck or a Rabbit? For Google Cloud Vision, it depends how the image is rotated. <a href=\"https://t.co/a30VzjEXVv\">pic.twitter.com/a30VzjEXVv</a></p>&mdash; Max Woolf (@minimaxir) <a href=\"https://twitter.com/minimaxir/status/1103676561809539072?ref_src=twsrc%5Etfw\">March 7, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nIs it a Duck or a Rabbit? For Google Cloud Vision, it depends how the image is rotated. pic.twitter.com/a30VzjEXVv— Max Woolf (@minimaxir) March 7, 2019"
  },
  {
    "objectID": "index.html#changes",
    "href": "index.html#changes",
    "title": "Model Interpretation through Sensitivity Analysis for Segmentation",
    "section": "Changes",
    "text": "Changes\n\n0.1.0 <2022-07-14>\n\nRe-write internal function to use pillow instead of fastai (version 1)\n\n\n\n0.0.4 <2021-01-14>\n\nInitial release"
  }
]